{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7425b373",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8182e221",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1818c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"figure.figsize\": (14, 7),\n",
    "        \"font.size\": 11,\n",
    "        \"axes.titlesize\": 14,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"figure.dpi\": 100,\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"font.family\": \"sans-serif\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Professional color palettes - using darker, more distinguishable colors\n",
    "model_colors = sns.color_palette(\"dark\", 8)\n",
    "benchmark_colors = sns.color_palette(\"dark\", 10)  # Darker palette for better visibility\n",
    "method_palette = {\n",
    "    \"Baseline\": \"#2C3E50\",\n",
    "    \"Thinkless\": \"#E74C3C\",\n",
    "    \"ThinkBrake\": \"#3498DB\",\n",
    "}\n",
    "threshold_palette = sns.color_palette(\"Blues\", 6)[1:]  # Skip lightest\n",
    "bar_colors = [\n",
    "    \"#2C3E50\",\n",
    "    \"#3498DB\",\n",
    "    \"#E67E22\",\n",
    "    \"#27AE60\",\n",
    "    \"#9B59B6\",\n",
    "    \"#1ABC9C\",\n",
    "    \"#E74C3C\",\n",
    "    \"#95A5A6\",\n",
    "]\n",
    "\n",
    "# Darker, more distinguishable line colors\n",
    "line_colors = [\n",
    "    \"#1A5276\",  # Dark blue\n",
    "    \"#B03A2E\",  # Dark red\n",
    "    \"#1E8449\",  # Dark green\n",
    "    \"#7D3C98\",  # Dark purple\n",
    "    \"#B9770E\",  # Dark orange\n",
    "    \"#117A65\",  # Dark teal\n",
    "    \"#6C3483\",  # Purple\n",
    "    \"#1F618D\",  # Steel blue\n",
    "]\n",
    "\n",
    "markers = [\"o\", \"s\", \"^\", \"D\", \"v\", \"p\", \"h\", \"*\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ebe8ab",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef1bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_subplot_layout(n_items, max_cols=4):\n",
    "    if n_items == 0:\n",
    "        return 1, 1, (6, 4)\n",
    "\n",
    "    cols = min(n_items, max_cols)\n",
    "    rows = math.ceil(n_items / cols)\n",
    "    fig_width = 5.5 * cols\n",
    "    fig_height = 4.5 * rows\n",
    "    return rows, cols, (fig_width, fig_height)\n",
    "\n",
    "\n",
    "def create_dynamic_subplots(n_items, max_cols=4, sharey=False):\n",
    "    rows, cols, figsize = calc_subplot_layout(n_items, max_cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize, sharey=sharey)\n",
    "\n",
    "    if n_items == 1:\n",
    "        axes = np.array([axes])\n",
    "    else:\n",
    "        axes = np.array(axes).flatten()\n",
    "\n",
    "    for i in range(n_items, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "def add_value_labels(ax, bars, fmt=\".1f\", rotation=0, fontsize=9, offset=0.5):\n",
    "    \"\"\"Add value labels on top of bars\"\"\"\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height > 0:\n",
    "            ax.annotate(\n",
    "                f\"{height:{fmt}}\",\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, offset),\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=fontsize,\n",
    "                rotation=rotation,\n",
    "            )\n",
    "\n",
    "\n",
    "def format_model_name(name, short_names):\n",
    "    \"\"\"Format model name for display\"\"\"\n",
    "    return short_names.get(name, name.split(\"/\")[-1])\n",
    "\n",
    "\n",
    "def load_detailed_results(model_dir, benchmark, method=\"rollout\"):\n",
    "    \"\"\"Load detailed results from JSONL files\"\"\"\n",
    "    base_path = Path(f\"../outputs/{model_dir}\")\n",
    "\n",
    "    # Determine category (math or general)\n",
    "    math_benchmarks = [\"gsm8k\", \"math500\", \"aime2024\", \"aime2025\"]\n",
    "    general_benchmarks = [\"gpqa-diamond\"]\n",
    "\n",
    "    if benchmark in math_benchmarks:\n",
    "        category = \"math\"\n",
    "    elif benchmark in general_benchmarks:\n",
    "        category = \"general\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    if method == \"rollout\":\n",
    "        file_path = base_path / category / \"rollout\" / f\"{benchmark}_result.jsonl\"\n",
    "    else:\n",
    "        # For thinkbrake, method should be like \"threshold_1.0\"\n",
    "        file_path = (\n",
    "            base_path / category / \"thinkbrake\" / method / f\"{benchmark}_result.jsonl\"\n",
    "        )\n",
    "\n",
    "    if not file_path.exists():\n",
    "        return None\n",
    "\n",
    "    results = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                results.append(json.loads(line))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314f45d2",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../outputs/leaderboard_rollout.json\", \"r\") as f:\n",
    "    rollout_data = json.load(f)\n",
    "\n",
    "with open(\"../outputs/leaderboard_thinkbrake.json\", \"r\") as f:\n",
    "    thinkbrake_data = json.load(f)\n",
    "\n",
    "\n",
    "INCLUDE_MODELS = [\n",
    "    \"Qwen_Qwen3-4B-Thinking-2507\",\n",
    "    \"Qwen_Qwen3-4B\",\n",
    "    \"Qwen_Qwen3-14B\",\n",
    "    \"Qwen_Qwen3-32B\",\n",
    "    \"deepseek-ai_DeepSeek-R1-Distill-Llama-8B\",\n",
    "    \"deepseek-ai_DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    \"openai_gpt-oss-20b\",\n",
    "    \"microsoft_phi-4-reasoning\",\n",
    "]\n",
    "\n",
    "INCLUDE_BENCHMARKS = [\n",
    "    \"gsm8k\",\n",
    "    \"math500\",\n",
    "    \"aime2024\",\n",
    "    \"aime2025\",\n",
    "    \"gpqa-diamond\",\n",
    "    \"mmlu-redux\",\n",
    "]\n",
    "\n",
    "INCLUDE_THRESHOLDS = [\n",
    "    # \"0.0\",\n",
    "    \"0.1\",\n",
    "    \"0.25\",\n",
    "    \"1.0\",\n",
    "    \"2.5\",\n",
    "    # \"5.0\",\n",
    "]\n",
    "\n",
    "model_name_map = {\n",
    "    \"Qwen_Qwen3-4B-Thinking-2507\": \"Qwen/Qwen3-4B-Thinking-2507\",\n",
    "    \"Qwen_Qwen3-4B\": \"Qwen/Qwen3-4B\",\n",
    "    \"Qwen_Qwen3-14B\": \"Qwen/Qwen3-14B\",\n",
    "    \"Qwen_Qwen3-32B\": \"Qwen/Qwen3-32B\",\n",
    "    \"deepseek-ai_DeepSeek-R1-Distill-Llama-8B\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    \"deepseek-ai_DeepSeek-R1-Distill-Qwen-7B\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    \"openai_gpt-oss-20b\": \"openai/gpt-oss-20b\",\n",
    "    \"microsoft_phi-4-reasoning\": \"microsoft/phi-4-reasoning\",\n",
    "}\n",
    "\n",
    "short_names = {\n",
    "    \"Qwen/Qwen3-4B-Thinking-2507\": \"Qwen3-4B-2507\",\n",
    "    \"Qwen/Qwen3-4B\": \"Qwen3-4B\",\n",
    "    \"Qwen/Qwen3-14B\": \"Qwen3-14B\",\n",
    "    \"Qwen/Qwen3-32B\": \"Qwen3-32B\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\": \"DS-R1-8B\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\": \"DS-R1-7B\",\n",
    "    \"openai/gpt-oss-20b\": \"GPT-OSS-20B\",\n",
    "    \"microsoft/phi-4-reasoning\": \"Phi-4-Reasoning\",\n",
    "}\n",
    "\n",
    "benchmarks = INCLUDE_BENCHMARKS\n",
    "thresholds = sorted(INCLUDE_THRESHOLDS, key=lambda x: float(x))\n",
    "\n",
    "filtered_rollout_data = {k: v for k, v in rollout_data.items() if k in INCLUDE_MODELS}\n",
    "filtered_thinkbrake_data = {\n",
    "    k: v\n",
    "    for k, v in thinkbrake_data.items()\n",
    "    if k in [model_name_map.get(m, m) for m in INCLUDE_MODELS]\n",
    "}\n",
    "baseline_data = filtered_rollout_data\n",
    "\n",
    "print(\"âœ… Data loaded and filtered successfully!\")\n",
    "print(f\"Included models (rollout): {list(filtered_rollout_data.keys())}\")\n",
    "print(f\"Included models (thinkbrake): {list(filtered_thinkbrake_data.keys())}\")\n",
    "print(f\"Included benchmarks: {benchmarks}\")\n",
    "print(f\"Included thresholds: {thresholds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df36d949",
   "metadata": {},
   "source": [
    "# Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8530d17",
   "metadata": {},
   "source": [
    "## Average Performance by Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b6a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark sample counts for weighted average\n",
    "BENCHMARK_SAMPLES = {\n",
    "    \"gsm8k\": 1319,\n",
    "    \"math500\": 500,\n",
    "    \"aime2024\": 30,\n",
    "    \"aime2025\": 30,\n",
    "    \"gpqa-diamond\": 198,\n",
    "    \"mmlu-redux\": 5700,\n",
    "}\n",
    "\n",
    "# Benchmark categories\n",
    "MATH_BENCHMARKS = [\"gsm8k\", \"math500\", \"aime2024\", \"aime2025\"]\n",
    "GENERAL_BENCHMARKS = [\"gpqa-diamond\", \"mmlu-redux\"]\n",
    "\n",
    "\n",
    "def compute_weighted_avg(accs, weights):\n",
    "    \"\"\"Compute weighted average\"\"\"\n",
    "    if not accs:\n",
    "        return None\n",
    "    return np.average(accs, weights=weights)\n",
    "\n",
    "\n",
    "def collect_category_data(\n",
    "    model_data, thinkbrake_model_data, category_benchmarks, thresholds\n",
    "):\n",
    "    \"\"\"Collect accuracy data for a specific category of benchmarks\"\"\"\n",
    "    baseline_accs, baseline_weights = [], []\n",
    "    threshold_data = {t: {\"accs\": [], \"weights\": []} for t in thresholds}\n",
    "\n",
    "    for b in category_benchmarks:\n",
    "        if b in model_data:\n",
    "            baseline_accs.append(model_data[b][\"accuracy\"])\n",
    "            baseline_weights.append(BENCHMARK_SAMPLES.get(b, 1))\n",
    "\n",
    "            if thinkbrake_model_data and b in thinkbrake_model_data:\n",
    "                for thresh in thresholds:\n",
    "                    key = f\"threshold_{thresh}\"\n",
    "                    if key in thinkbrake_model_data[b]:\n",
    "                        threshold_data[thresh][\"accs\"].append(\n",
    "                            thinkbrake_model_data[b][key][\"accuracy\"]\n",
    "                        )\n",
    "                        threshold_data[thresh][\"weights\"].append(\n",
    "                            BENCHMARK_SAMPLES.get(b, 1)\n",
    "                        )\n",
    "\n",
    "    return baseline_accs, baseline_weights, threshold_data\n",
    "\n",
    "\n",
    "# Collect data for all categories\n",
    "avg_data_by_category = {\"Math\": [], \"General\": [], \"All\": []}\n",
    "\n",
    "for rollout_name, rollout_model_data in filtered_rollout_data.items():\n",
    "    try:\n",
    "        mapped_name = model_name_map.get(rollout_name, rollout_name)\n",
    "        display_name = short_names.get(mapped_name, rollout_name)\n",
    "        thinkbrake_model_data = filtered_thinkbrake_data.get(mapped_name, None)\n",
    "\n",
    "        for category, cat_benchmarks in [\n",
    "            (\"Math\", MATH_BENCHMARKS),\n",
    "            (\"General\", GENERAL_BENCHMARKS),\n",
    "            (\"All\", benchmarks),\n",
    "        ]:\n",
    "            baseline_accs, baseline_weights, threshold_data = collect_category_data(\n",
    "                rollout_model_data, thinkbrake_model_data, cat_benchmarks, thresholds\n",
    "            )\n",
    "\n",
    "            if baseline_accs:\n",
    "                baseline_avg = compute_weighted_avg(baseline_accs, baseline_weights)\n",
    "                avg_data_by_category[category].append(\n",
    "                    {\n",
    "                        \"model\": display_name,\n",
    "                        \"type\": \"Baseline\",\n",
    "                        \"avg_accuracy\": baseline_avg,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                for thresh in thresholds:\n",
    "                    if threshold_data[thresh][\"accs\"]:\n",
    "                        avg_data_by_category[category].append(\n",
    "                            {\n",
    "                                \"model\": display_name,\n",
    "                                \"type\": f\"t={thresh}\",\n",
    "                                \"avg_accuracy\": compute_weighted_avg(\n",
    "                                    threshold_data[thresh][\"accs\"],\n",
    "                                    threshold_data[thresh][\"weights\"],\n",
    "                                ),\n",
    "                            }\n",
    "                        )\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# Create 3 subplots: Math, General, All\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 18))\n",
    "\n",
    "for ax_idx, (category, avg_data) in enumerate(avg_data_by_category.items()):\n",
    "    ax = axes[ax_idx]\n",
    "\n",
    "    if not avg_data:\n",
    "        ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\")\n",
    "        continue\n",
    "\n",
    "    df_avg = pd.DataFrame(avg_data)\n",
    "    models = df_avg[\"model\"].unique()\n",
    "    types = df_avg[\"type\"].unique()\n",
    "    x = np.arange(len(models))\n",
    "    n_types = len(types)\n",
    "    width = 0.8 / n_types\n",
    "\n",
    "    for i, t in enumerate(types):\n",
    "        type_data = df_avg[df_avg[\"type\"] == t]\n",
    "        values = [\n",
    "            (\n",
    "                type_data[type_data[\"model\"] == m][\"avg_accuracy\"].values[0]\n",
    "                if len(type_data[type_data[\"model\"] == m]) > 0\n",
    "                else 0\n",
    "            )\n",
    "            for m in models\n",
    "        ]\n",
    "        bars = ax.bar(\n",
    "            x + (i - n_types / 2 + 0.5) * width,\n",
    "            values,\n",
    "            width * 0.9,\n",
    "            label=t,\n",
    "            color=bar_colors[i % len(bar_colors)],\n",
    "            edgecolor=\"white\",\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "\n",
    "        # Get baseline values for comparison\n",
    "        baseline_data_df = df_avg[df_avg[\"type\"] == \"Baseline\"]\n",
    "        baseline_values = {\n",
    "            m: (\n",
    "                baseline_data_df[baseline_data_df[\"model\"] == m][\"avg_accuracy\"].values[\n",
    "                    0\n",
    "                ]\n",
    "                if len(baseline_data_df[baseline_data_df[\"model\"] == m]) > 0\n",
    "                else 0\n",
    "            )\n",
    "            for m in models\n",
    "        }\n",
    "\n",
    "        for bar, val, m in zip(bars, values, models):\n",
    "            if val > 0:\n",
    "                is_better = t != \"Baseline\" and val > baseline_values.get(m, 0)\n",
    "                ax.text(\n",
    "                    bar.get_x() + bar.get_width() / 2,\n",
    "                    bar.get_height() + 0.3,\n",
    "                    f\"{val:.1f}%\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    fontsize=6,\n",
    "                    color=\"red\" if is_better else \"#333333\",\n",
    "                    fontweight=\"bold\" if is_better else \"normal\",\n",
    "                    rotation=90,\n",
    "                )\n",
    "\n",
    "    # Category-specific info\n",
    "    if category == \"Math\":\n",
    "        cat_benchmarks = MATH_BENCHMARKS\n",
    "        subtitle = \"gsm8k, math500, aime2024, aime2025\"\n",
    "    elif category == \"General\":\n",
    "        cat_benchmarks = GENERAL_BENCHMARKS\n",
    "        subtitle = \"gpqa-diamond, mmlu-redux\"\n",
    "    else:\n",
    "        cat_benchmarks = benchmarks\n",
    "        subtitle = \"All benchmarks\"\n",
    "\n",
    "    total_samples = sum(BENCHMARK_SAMPLES[b] for b in cat_benchmarks)\n",
    "\n",
    "    ax.set_ylabel(\"Weighted Avg Accuracy (%)\")\n",
    "    ax.set_title(\n",
    "        f\"{category} Benchmarks\\n({subtitle}, n={total_samples:,})\",\n",
    "        fontsize=11,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models, rotation=25, ha=\"right\", fontsize=8)\n",
    "    ax.set_ylim(0, max(df_avg[\"avg_accuracy\"]) * 1.2)\n",
    "    ax.grid(True, alpha=0.3, axis=\"y\", linestyle=\"--\")\n",
    "    ax.set_facecolor(\"#FAFAFA\")\n",
    "\n",
    "# Single legend for all subplots\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1.01, 0.5),\n",
    "    framealpha=0.9,\n",
    "    ncol=1,\n",
    "    title=\"Method\",\n",
    ")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Weighted Average Accuracy by Category\", fontsize=14, fontweight=\"bold\", y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\nðŸ“Š Category Sample Breakdown:\")\n",
    "print(f\"  Math: {sum(BENCHMARK_SAMPLES[b] for b in MATH_BENCHMARKS):,} samples\")\n",
    "print(f\"  General: {sum(BENCHMARK_SAMPLES[b] for b in GENERAL_BENCHMARKS):,} samples\")\n",
    "print(f\"  Total: {sum(BENCHMARK_SAMPLES.values()):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40de0e0",
   "metadata": {},
   "source": [
    "## Token Savings across Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec24b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = len(filtered_rollout_data)\n",
    "if num_models == 0:\n",
    "    print(\"No data available for this plot.\")\n",
    "else:\n",
    "    fig, axes = create_dynamic_subplots(num_models, max_cols=3)\n",
    "\n",
    "    for model_idx, (rollout_name, rollout_model_data) in enumerate(\n",
    "        filtered_rollout_data.items()\n",
    "    ):\n",
    "        ax = axes[model_idx]\n",
    "        mapped_name = model_name_map.get(rollout_name, rollout_name)\n",
    "        display_name = short_names.get(mapped_name, rollout_name)\n",
    "\n",
    "        for bench_idx, benchmark in enumerate(benchmarks):\n",
    "            try:\n",
    "                if benchmark not in rollout_model_data:\n",
    "                    continue\n",
    "\n",
    "                baseline_tokens = rollout_model_data[benchmark][\"avg_token_length\"]\n",
    "\n",
    "                if (\n",
    "                    mapped_name in filtered_thinkbrake_data\n",
    "                    and benchmark in filtered_thinkbrake_data[mapped_name]\n",
    "                ):\n",
    "                    tb_data = filtered_thinkbrake_data[mapped_name][benchmark]\n",
    "                    x_vals = [\"Baseline\"]\n",
    "                    y_vals = [0]\n",
    "\n",
    "                    for thresh in thresholds:\n",
    "                        key = f\"threshold_{thresh}\"\n",
    "                        if key in tb_data:\n",
    "                            x_vals.append(f\"t={thresh}\")\n",
    "                            token_savings = (\n",
    "                                1 - tb_data[key][\"avg_token_length\"] / baseline_tokens\n",
    "                            ) * 100\n",
    "                            y_vals.append(token_savings)\n",
    "\n",
    "                    color = line_colors[bench_idx % len(line_colors)]\n",
    "                    ax.plot(\n",
    "                        x_vals,\n",
    "                        y_vals,\n",
    "                        marker=markers[bench_idx % len(markers)],\n",
    "                        linewidth=2.5,\n",
    "                        markersize=10,\n",
    "                        color=color,\n",
    "                        label=benchmark,\n",
    "                        markeredgecolor=\"white\",\n",
    "                        markeredgewidth=1.5,\n",
    "                    )\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        ax.axhline(y=0, color=\"#BDC3C7\", linestyle=\"--\", alpha=0.8, linewidth=1.5)\n",
    "        ax.set_ylabel(\"Token Savings (%)\")\n",
    "        ax.set_title(f\"{display_name}\", fontsize=12, pad=10)\n",
    "        ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "        ax.tick_params(axis=\"x\", rotation=0)\n",
    "        ax.set_facecolor(\"#FAFAFA\")\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        ncol=1,\n",
    "        fontsize=10,\n",
    "        framealpha=0.9,\n",
    "    )\n",
    "    plt.suptitle(\n",
    "        \"Token Savings by Benchmark\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        y=1.00,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.08, hspace=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb78c9",
   "metadata": {},
   "source": [
    "## Per-Model Bar Chart: Accuracy by Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806951a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-model bar chart: Accuracy by benchmark\n",
    "num_models = len(filtered_rollout_data)\n",
    "if num_models == 0:\n",
    "    print(\"No data available for this plot.\")\n",
    "else:\n",
    "    methods = [\"Baseline\"] + [f\"t={t}\" for t in thresholds]\n",
    "\n",
    "    # Create a combined figure with subplots for each model\n",
    "    fig, axes = plt.subplots(num_models, 1, figsize=(16, 5 * num_models))\n",
    "    if num_models == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for model_idx, (rollout_name, rollout_model_data) in enumerate(\n",
    "        filtered_rollout_data.items()\n",
    "    ):\n",
    "        ax = axes[model_idx]\n",
    "        mapped_name = model_name_map.get(rollout_name, rollout_name)\n",
    "        display_name = short_names.get(mapped_name, rollout_name)\n",
    "\n",
    "        model_benchmarks = []\n",
    "        model_accuracies = {m: [] for m in methods}\n",
    "\n",
    "        for benchmark in benchmarks:\n",
    "            try:\n",
    "                if benchmark not in rollout_model_data:\n",
    "                    continue\n",
    "\n",
    "                model_benchmarks.append(benchmark)\n",
    "                model_accuracies[\"Baseline\"].append(\n",
    "                    rollout_model_data[benchmark][\"accuracy\"]\n",
    "                )\n",
    "\n",
    "                if (\n",
    "                    mapped_name in filtered_thinkbrake_data\n",
    "                    and benchmark in filtered_thinkbrake_data[mapped_name]\n",
    "                ):\n",
    "                    tb_data = filtered_thinkbrake_data[mapped_name][benchmark]\n",
    "                    for thresh in thresholds:\n",
    "                        key = f\"threshold_{thresh}\"\n",
    "                        if key in tb_data:\n",
    "                            model_accuracies[f\"t={thresh}\"].append(\n",
    "                                tb_data[key][\"accuracy\"]\n",
    "                            )\n",
    "                        else:\n",
    "                            model_accuracies[f\"t={thresh}\"].append(0)\n",
    "                else:\n",
    "                    for t in thresholds:\n",
    "                        model_accuracies[f\"t={t}\"].append(0)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if model_benchmarks:\n",
    "            x = np.arange(len(model_benchmarks))\n",
    "            n_methods = len(methods)\n",
    "            width = 0.8 / n_methods\n",
    "\n",
    "            for i, method in enumerate(methods):\n",
    "                try:\n",
    "                    if model_accuracies[method]:\n",
    "                        bars = ax.bar(\n",
    "                            x + (i - n_methods / 2 + 0.5) * width,\n",
    "                            model_accuracies[method],\n",
    "                            width * 0.9,\n",
    "                            label=method,\n",
    "                            color=bar_colors[i % len(bar_colors)],\n",
    "                            edgecolor=\"white\",\n",
    "                            linewidth=0.5,\n",
    "                        )\n",
    "                        # Add percentage labels on bars\n",
    "                        for bar_idx, (bar, val) in enumerate(\n",
    "                            zip(bars, model_accuracies[method])\n",
    "                        ):\n",
    "                            if val > 0:\n",
    "                                # Check if this threshold beats baseline\n",
    "                                baseline_val = model_accuracies[\"Baseline\"][bar_idx]\n",
    "                                if method != \"Baseline\" and val >= baseline_val:\n",
    "                                    text_color = (\n",
    "                                        \"#E74C3C\"  # Red for better than baseline\n",
    "                                    )\n",
    "                                    fontweight = \"bold\"\n",
    "                                else:\n",
    "                                    text_color = \"#333333\"\n",
    "                                    fontweight = \"normal\"\n",
    "                                ax.text(\n",
    "                                    bar.get_x() + bar.get_width() / 2,\n",
    "                                    bar.get_height() + 0.5,\n",
    "                                    f\"{val:.1f}%\",\n",
    "                                    ha=\"center\",\n",
    "                                    va=\"bottom\",\n",
    "                                    fontsize=7,\n",
    "                                    color=text_color,\n",
    "                                    fontweight=fontweight,\n",
    "                                    rotation=90,\n",
    "                                )\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            ax.set_ylabel(\"Accuracy (%)\")\n",
    "            ax.set_title(f\"{display_name}\", fontsize=14, pad=10)\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(model_benchmarks, rotation=0, ha=\"center\")\n",
    "            # Move legend outside the plot (to the right)\n",
    "            ax.legend(\n",
    "                loc=\"center left\",\n",
    "                bbox_to_anchor=(1.02, 0.5),\n",
    "                fontsize=9,\n",
    "                framealpha=0.9,\n",
    "                ncol=1,\n",
    "                title=\"Method\",\n",
    "            )\n",
    "            ax.grid(True, alpha=0.3, axis=\"y\", linestyle=\"--\")\n",
    "            ax.set_ylim(0, 115)\n",
    "            ax.set_facecolor(\"#FAFAFA\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2069ce20",
   "metadata": {},
   "source": [
    "## Load Detailed Results Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd7582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_detailed_results(model_dirs, benchmarks, best_threshold=\"0.25\"):\n",
    "    all_results = {}\n",
    "\n",
    "    for model_dir in model_dirs:\n",
    "        all_results[model_dir] = {}\n",
    "        for benchmark in benchmarks:\n",
    "            rollout_results = load_detailed_results(model_dir, benchmark, \"rollout\")\n",
    "            thinkbrake_results = load_detailed_results(\n",
    "                model_dir, benchmark, f\"threshold_{best_threshold}\"\n",
    "            )\n",
    "\n",
    "            if rollout_results and thinkbrake_results:\n",
    "                all_results[model_dir][benchmark] = {\n",
    "                    \"rollout\": rollout_results,\n",
    "                    \"thinkbrake\": thinkbrake_results,\n",
    "                }\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8511bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_THRESHOLD = 0.1\n",
    "detailed_results = load_all_detailed_results(\n",
    "    INCLUDE_MODELS, INCLUDE_BENCHMARKS, BEST_THRESHOLD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d1f9e1",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537fb1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_verify import parse, verify\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_multiple_choice_answer(response: str) -> str:\n",
    "    patterns = [\n",
    "        r'[\"\\*]*answer[\"\\*]*\\s*[:=]\\s*[\"\\']?([A-Da-d])[\"\\']?',\n",
    "        r\"(?:the\\s+)?answer\\s+is[:\\s]*([A-Da-d])\\b\",\n",
    "        r\"final\\s+answer[:\\s]*([A-Da-d])\\b\",\n",
    "        r\"(?:choice|option)[:\\s]*([A-Da-d])\\b\",\n",
    "        r\"\\b([A-Da-d])\\s*$\",\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, response, re.IGNORECASE)\n",
    "        if matches:\n",
    "            return matches[-1].upper()\n",
    "\n",
    "    standalone_matches = re.findall(r\"\\b([A-Da-d])\\b\", response)\n",
    "    if standalone_matches:\n",
    "        return standalone_matches[-1].upper()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def verify_multiple_choice(ground_truth: str, predicted: str) -> bool:\n",
    "    \"\"\"Verify if the predicted multiple choice answer matches the ground truth.\"\"\"\n",
    "    if not predicted:\n",
    "        return False\n",
    "    return ground_truth.upper().strip() == predicted.upper().strip()\n",
    "\n",
    "\n",
    "def evaluate_item(item: dict, is_multiple_choice: bool) -> bool:\n",
    "    \"\"\"\n",
    "    Evaluate a single item using the same logic as evaluate.py\n",
    "    \"\"\"\n",
    "    if is_multiple_choice:\n",
    "        ground_truth = item[\"answer\"]\n",
    "        predicted = extract_multiple_choice_answer(item[\"response\"])\n",
    "        return verify_multiple_choice(ground_truth, predicted)\n",
    "    else:\n",
    "        # Math evaluation using math_verify\n",
    "        try:\n",
    "            ground_truth = parse(f\"${item['answer']}$\")\n",
    "            predicted = parse(item[\"response\"])\n",
    "            return verify(ground_truth, predicted)\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "\n",
    "def compute_confusion_matrix(rollout_results, thinkbrake_results, benchmark):\n",
    "    \"\"\"Compute confusion matrix comparing rollout (baseline) vs thinkbrake results\"\"\"\n",
    "    rollout_dict = {r[\"id\"]: r for r in rollout_results}\n",
    "    thinkbrake_dict = {r[\"id\"]: r for r in thinkbrake_results}\n",
    "\n",
    "    # Determine if it's a multiple choice benchmark\n",
    "    general_benchmarks = [\"gpqa-diamond\", \"mmlu-redux\"]\n",
    "    is_multiple_choice = benchmark in general_benchmarks\n",
    "\n",
    "    both_correct = 0\n",
    "    only_rollout_correct = 0\n",
    "    only_thinkbrake_correct = 0\n",
    "    both_wrong = 0\n",
    "\n",
    "    # Find common IDs\n",
    "    common_ids = set(rollout_dict.keys()) & set(thinkbrake_dict.keys())\n",
    "\n",
    "    for pid in common_ids:\n",
    "        rollout_item = rollout_dict[pid]\n",
    "        tb_item = thinkbrake_dict[pid]\n",
    "\n",
    "        # Evaluate using the same method as evaluate.py\n",
    "        rollout_correct = evaluate_item(rollout_item, is_multiple_choice)\n",
    "        tb_correct = evaluate_item(tb_item, is_multiple_choice)\n",
    "\n",
    "        if rollout_correct and tb_correct:\n",
    "            both_correct += 1\n",
    "        elif rollout_correct and not tb_correct:\n",
    "            only_rollout_correct += 1\n",
    "        elif not rollout_correct and tb_correct:\n",
    "            only_thinkbrake_correct += 1\n",
    "        else:\n",
    "            both_wrong += 1\n",
    "\n",
    "    return (\n",
    "        both_correct,\n",
    "        only_rollout_correct,\n",
    "        only_thinkbrake_correct,\n",
    "        both_wrong,\n",
    "        len(common_ids),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6fc21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_data = {}\n",
    "\n",
    "for model_dir, benchmarks_data in detailed_results.items():\n",
    "    display_name = short_names.get(model_name_map.get(model_dir, model_dir), model_dir)\n",
    "    confusion_data[display_name] = {}\n",
    "\n",
    "    for benchmark, data in benchmarks_data.items():\n",
    "        if data[\"rollout\"] and data[\"thinkbrake\"]:\n",
    "            cm = compute_confusion_matrix(\n",
    "                data[\"rollout\"], data[\"thinkbrake\"], benchmark\n",
    "            )\n",
    "            confusion_data[display_name][benchmark] = {\n",
    "                \"both_correct\": cm[0],\n",
    "                \"only_rollout\": cm[1],\n",
    "                \"only_thinkbrake\": cm[2],\n",
    "                \"both_wrong\": cm[3],\n",
    "                \"total\": cm[4],\n",
    "            }\n",
    "\n",
    "\n",
    "n_models = len([m for m in confusion_data if confusion_data[m]])\n",
    "if n_models > 0:\n",
    "    fig, axes = create_dynamic_subplots(n_models, max_cols=3)\n",
    "\n",
    "    for idx, (model_name, bench_data) in enumerate(confusion_data.items()):\n",
    "        if not bench_data:\n",
    "            continue\n",
    "        ax = axes[idx]\n",
    "\n",
    "        total_both_correct = sum(d[\"both_correct\"] for d in bench_data.values())\n",
    "        total_only_rollout = sum(d[\"only_rollout\"] for d in bench_data.values())\n",
    "        total_only_tb = sum(d[\"only_thinkbrake\"] for d in bench_data.values())\n",
    "        total_both_wrong = sum(d[\"both_wrong\"] for d in bench_data.values())\n",
    "        total = sum(d[\"total\"] for d in bench_data.values())\n",
    "\n",
    "        # Calculate accuracies\n",
    "        baseline_acc = (\n",
    "            (total_both_correct + total_only_rollout) / total * 100 if total > 0 else 0\n",
    "        )\n",
    "        tb_acc = (total_both_correct + total_only_tb) / total * 100 if total > 0 else 0\n",
    "\n",
    "        # Create confusion matrix\n",
    "        # Rows: ThinkBrake (Correct/Wrong), Cols: Baseline (Correct/Wrong)\n",
    "        cm = np.array(\n",
    "            [\n",
    "                [\n",
    "                    total_both_correct,\n",
    "                    total_only_tb,\n",
    "                ],  # TB Correct: Both correct, Only TB correct\n",
    "                [\n",
    "                    total_only_rollout,\n",
    "                    total_both_wrong,\n",
    "                ],  # TB Wrong: Only Baseline correct, Both wrong\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Plot heatmap\n",
    "        im = ax.imshow(cm, cmap=\"Blues\")\n",
    "\n",
    "        # Add labels - fixed orientation\n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_xticklabels([\"Baseline âœ“\", \"Baseline âœ—\"])\n",
    "        ax.set_yticklabels([\"ThinkBrake âœ“\", \"ThinkBrake âœ—\"])\n",
    "        ax.set_xlabel(\"Baseline (Rollout)\", fontweight=\"bold\")\n",
    "        ax.set_ylabel(f\"ThinkBrake (t={BEST_THRESHOLD})\", fontweight=\"bold\")\n",
    "\n",
    "        # Add text annotations\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                pct = cm[i, j] / total * 100 if total > 0 else 0\n",
    "                text = ax.text(\n",
    "                    j,\n",
    "                    i,\n",
    "                    f\"{cm[i, j]}\\n({pct:.1f}%)\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=11,\n",
    "                    color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n",
    "                )\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"{model_name}\\nBaseline: {baseline_acc:.1f}% | TB: {tb_acc:.1f}%\",\n",
    "            fontsize=11,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Confusion Matrix\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        y=1.02,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd489b",
   "metadata": {},
   "source": [
    "## Confusion Matrix by Benchmark\n",
    "\n",
    "ê° ë²¤ì¹˜ë§ˆí¬ë³„ë¡œ Baselineê³¼ ThinkBrakeì˜ ì •ë‹µ ì¼ì¹˜/ë¶ˆì¼ì¹˜ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc28154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix by Benchmark for each model\n",
    "for model_dir, benchmarks_data in detailed_results.items():\n",
    "    display_name = short_names.get(model_name_map.get(model_dir, model_dir), model_dir)\n",
    "\n",
    "    # Get benchmarks that have data\n",
    "    available_benchmarks = [b for b in INCLUDE_BENCHMARKS if b in benchmarks_data]\n",
    "    n_benchmarks = len(available_benchmarks)\n",
    "\n",
    "    if n_benchmarks == 0:\n",
    "        continue\n",
    "\n",
    "    fig, axes = create_dynamic_subplots(n_benchmarks, max_cols=3)\n",
    "\n",
    "    for idx, benchmark in enumerate(available_benchmarks):\n",
    "        ax = axes[idx]\n",
    "        data = benchmarks_data[benchmark]\n",
    "\n",
    "        if not data[\"rollout\"] or not data[\"thinkbrake\"]:\n",
    "            continue\n",
    "\n",
    "        cm_result = compute_confusion_matrix(\n",
    "            data[\"rollout\"], data[\"thinkbrake\"], benchmark\n",
    "        )\n",
    "        both_correct, only_rollout, only_tb, both_wrong, total = cm_result\n",
    "\n",
    "        # Calculate accuracies\n",
    "        baseline_acc = (both_correct + only_rollout) / total * 100 if total > 0 else 0\n",
    "        tb_acc = (both_correct + only_tb) / total * 100 if total > 0 else 0\n",
    "        diff = tb_acc - baseline_acc\n",
    "\n",
    "        # Create confusion matrix\n",
    "        cm = np.array(\n",
    "            [\n",
    "                [both_correct, only_tb],\n",
    "                [only_rollout, both_wrong],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Plot heatmap\n",
    "        im = ax.imshow(cm, cmap=\"Blues\")\n",
    "\n",
    "        # Add labels\n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_xticklabels([\"Base âœ“\", \"Base âœ—\"], fontsize=9)\n",
    "        ax.set_yticklabels([\"TB âœ“\", \"TB âœ—\"], fontsize=9)\n",
    "\n",
    "        # Add text annotations\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                pct = cm[i, j] / total * 100 if total > 0 else 0\n",
    "                ax.text(\n",
    "                    j,\n",
    "                    i,\n",
    "                    f\"{cm[i, j]}\\n({pct:.1f}%)\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=9,\n",
    "                    color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n",
    "                )\n",
    "\n",
    "        # Title with accuracy comparison\n",
    "        diff_color = \"green\" if diff >= 0 else (\"red\" if diff < 0 else \"black\")\n",
    "        diff_sign = \"+\" if diff == 0 else \"\"\n",
    "        ax.set_title(\n",
    "            f\"{benchmark}\\nBase: {baseline_acc:.1f}% â†’ TB: {tb_acc:.1f}% ({diff_sign}{diff:.1f}%)\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "            color=diff_color,\n",
    "        )\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"{display_name} - Confusion Matrix by Benchmark (t={BEST_THRESHOLD})\",\n",
    "        fontsize=13,\n",
    "        fontweight=\"bold\",\n",
    "        y=1.02,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThinkBrake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
