{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7425b373",
   "metadata": {},
   "source": [
    "# ThinkBrake: Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8182e221",
   "metadata": {},
   "source": [
    "## Import and Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1818c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"figure.figsize\": (14, 7),\n",
    "        \"font.size\": 11,\n",
    "        \"axes.titlesize\": 14,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"figure.dpi\": 100,\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"font.family\": \"sans-serif\",\n",
    "    }\n",
    ")\n",
    "\n",
    "model_colors = sns.color_palette(\"dark\", 8)\n",
    "benchmark_colors = sns.color_palette(\"dark\", 10)\n",
    "method_palette = {\n",
    "    \"Baseline\": \"#2C3E50\",\n",
    "    \"Thinkless\": \"#E74C3C\",\n",
    "    \"ThinkBrake\": \"#3498DB\",\n",
    "}\n",
    "threshold_palette = sns.color_palette(\"Blues\", 6)[1:]\n",
    "bar_colors = [\n",
    "    \"#2C3E50\",\n",
    "    \"#3498DB\",\n",
    "    \"#E67E22\",\n",
    "    \"#27AE60\",\n",
    "    \"#9B59B6\",\n",
    "    \"#1ABC9C\",\n",
    "    \"#E74C3C\",\n",
    "    \"#95A5A6\",\n",
    "]\n",
    "\n",
    "line_colors = [\n",
    "    \"#1A5276\",\n",
    "    \"#B03A2E\",\n",
    "    \"#1E8449\",\n",
    "    \"#7D3C98\",\n",
    "    \"#B9770E\",\n",
    "    \"#117A65\",\n",
    "    \"#6C3483\",\n",
    "    \"#1F618D\",\n",
    "]\n",
    "\n",
    "markers = [\"o\", \"s\", \"^\", \"D\", \"v\", \"p\", \"h\", \"*\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef1bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_subplot_layout(n_items, max_cols=4):\n",
    "    if n_items == 0:\n",
    "        return 1, 1, (6, 4)\n",
    "\n",
    "    cols = min(n_items, max_cols)\n",
    "    rows = math.ceil(n_items / cols)\n",
    "    fig_width = 5.5 * cols\n",
    "    fig_height = 4.5 * rows\n",
    "    return rows, cols, (fig_width, fig_height)\n",
    "\n",
    "\n",
    "def create_dynamic_subplots(n_items, max_cols=4, sharey=False):\n",
    "    rows, cols, figsize = calc_subplot_layout(n_items, max_cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize, sharey=sharey)\n",
    "\n",
    "    if n_items == 1:\n",
    "        axes = np.array([axes])\n",
    "    else:\n",
    "        axes = np.array(axes).flatten()\n",
    "\n",
    "    for i in range(n_items, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "def add_value_labels(ax, bars, fmt=\".1f\", rotation=0, fontsize=9, offset=0.5):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height > 0:\n",
    "            ax.annotate(\n",
    "                f\"{height:{fmt}}\",\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, offset),\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=fontsize,\n",
    "                rotation=rotation,\n",
    "            )\n",
    "\n",
    "\n",
    "def format_model_name(name, short_names):\n",
    "    return short_names.get(name, name.split(\"/\")[-1])\n",
    "\n",
    "\n",
    "def load_detailed_results(model_dir, benchmark, method=\"rollout\"):\n",
    "    base_path = Path(f\"../outputs/{model_dir}\")\n",
    "    math_benchmarks = [\"gsm8k\", \"math500\", \"aime2024\", \"aime2025\"]\n",
    "    general_benchmarks = [\"gpqa-diamond\"]\n",
    "    tool_benchmarks = [\"bfcl-v1\", \"bfcl-v2\"]\n",
    "\n",
    "    if benchmark in math_benchmarks:\n",
    "        category = \"math\"\n",
    "    elif benchmark in general_benchmarks:\n",
    "        category = \"general\"\n",
    "    elif benchmark in tool_benchmarks:\n",
    "        category = \"tool\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    if method == \"rollout\":\n",
    "        file_path = base_path / category / \"rollout\" / f\"{benchmark}_result.jsonl\"\n",
    "    else:\n",
    "        file_path = (\n",
    "            base_path / category / \"thinkbrake\" / method / f\"{benchmark}_result.jsonl\"\n",
    "        )\n",
    "\n",
    "    if not file_path.exists():\n",
    "        return None\n",
    "\n",
    "    results = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                results.append(json.loads(line))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314f45d2",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b05687",
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_MODELS = [\n",
    "    \"Qwen_Qwen3-4B-Thinking-2507\",\n",
    "    # \"Qwen_Qwen3-4B\",\n",
    "    # \"Qwen_Qwen3-14B\",\n",
    "    # \"Qwen_Qwen3-32B\",\n",
    "    \"deepseek-ai_DeepSeek-R1-Distill-Llama-8B\",\n",
    "    # \"deepseek-ai_DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    # \"microsoft_phi-4-reasoning\",\n",
    "]\n",
    "\n",
    "INCLUDE_BENCHMARKS = [\n",
    "    # \"gsm8k\",\n",
    "    # \"math500\",\n",
    "    # \"aime2024\",\n",
    "    # \"aime2025\",\n",
    "    # \"gpqa-diamond\",\n",
    "    \"bfcl-v1\",\n",
    "    \"bfcl-v2\",\n",
    "    # \"mmlu-redux\",\n",
    "]\n",
    "\n",
    "INCLUDE_THRESHOLDS = [\n",
    "    # \"0.0\",\n",
    "    \"0.1\",\n",
    "    \"0.25\",\n",
    "    \"1.0\",\n",
    "    \"2.5\",\n",
    "    # \"5.0\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../outputs/leaderboard_rollout.json\", \"r\") as f:\n",
    "    rollout_data = json.load(f)\n",
    "\n",
    "with open(\"../outputs/leaderboard_thinkbrake.json\", \"r\") as f:\n",
    "    thinkbrake_data = json.load(f)\n",
    "\n",
    "model_name_map = {\n",
    "    \"Qwen_Qwen3-4B-Thinking-2507\": \"Qwen/Qwen3-4B-Thinking-2507\",\n",
    "    \"Qwen_Qwen3-4B\": \"Qwen/Qwen3-4B\",\n",
    "    \"Qwen_Qwen3-14B\": \"Qwen/Qwen3-14B\",\n",
    "    \"Qwen_Qwen3-32B\": \"Qwen/Qwen3-32B\",\n",
    "    \"deepseek-ai_DeepSeek-R1-Distill-Llama-8B\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    \"deepseek-ai_DeepSeek-R1-Distill-Qwen-7B\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    \"microsoft_phi-4-reasoning\": \"microsoft/phi-4-reasoning\",\n",
    "}\n",
    "\n",
    "short_names = {\n",
    "    \"Qwen/Qwen3-4B-Thinking-2507\": \"Qwen3-4B-2507\",\n",
    "    \"Qwen/Qwen3-4B\": \"Qwen3-4B\",\n",
    "    \"Qwen/Qwen3-14B\": \"Qwen3-14B\",\n",
    "    \"Qwen/Qwen3-32B\": \"Qwen3-32B\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\": \"DS-R1-8B\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\": \"DS-R1-7B\",\n",
    "    \"microsoft/phi-4-reasoning\": \"Phi-4-Reasoning\",\n",
    "}\n",
    "\n",
    "benchmarks = INCLUDE_BENCHMARKS\n",
    "thresholds = sorted(INCLUDE_THRESHOLDS, key=lambda x: float(x))\n",
    "\n",
    "filtered_rollout_data = {k: v for k, v in rollout_data.items() if k in INCLUDE_MODELS}\n",
    "filtered_thinkbrake_data = {\n",
    "    k: v\n",
    "    for k, v in thinkbrake_data.items()\n",
    "    if k in [model_name_map.get(m, m) for m in INCLUDE_MODELS]\n",
    "}\n",
    "baseline_data = filtered_rollout_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df36d949",
   "metadata": {},
   "source": [
    "# Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40de0e0",
   "metadata": {},
   "source": [
    "## Token Savings across Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec24b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = len(filtered_rollout_data)\n",
    "if num_models == 0:\n",
    "    print(\"No data available for this plot.\")\n",
    "else:\n",
    "    fig, axes = create_dynamic_subplots(num_models, max_cols=3)\n",
    "\n",
    "    for model_idx, (rollout_name, rollout_model_data) in enumerate(\n",
    "        filtered_rollout_data.items()\n",
    "    ):\n",
    "        ax = axes[model_idx]\n",
    "        mapped_name = model_name_map.get(rollout_name, rollout_name)\n",
    "        display_name = short_names.get(mapped_name, rollout_name)\n",
    "\n",
    "        for bench_idx, benchmark in enumerate(benchmarks):\n",
    "            try:\n",
    "                if benchmark not in rollout_model_data:\n",
    "                    continue\n",
    "\n",
    "                baseline_tokens = rollout_model_data[benchmark][\"avg_token_length\"]\n",
    "\n",
    "                if (\n",
    "                    mapped_name in filtered_thinkbrake_data\n",
    "                    and benchmark in filtered_thinkbrake_data[mapped_name]\n",
    "                ):\n",
    "                    tb_data = filtered_thinkbrake_data[mapped_name][benchmark]\n",
    "                    x_vals = [\"Baseline\"]\n",
    "                    y_vals = [0]\n",
    "\n",
    "                    for thresh in thresholds:\n",
    "                        key = f\"threshold_{thresh}\"\n",
    "                        if key in tb_data:\n",
    "                            x_vals.append(f\"t={thresh}\")\n",
    "                            token_savings = (\n",
    "                                1 - tb_data[key][\"avg_token_length\"] / baseline_tokens\n",
    "                            ) * 100\n",
    "                            y_vals.append(token_savings)\n",
    "\n",
    "                    color = line_colors[bench_idx % len(line_colors)]\n",
    "                    ax.plot(\n",
    "                        x_vals,\n",
    "                        y_vals,\n",
    "                        marker=markers[bench_idx % len(markers)],\n",
    "                        linewidth=2.5,\n",
    "                        markersize=10,\n",
    "                        color=color,\n",
    "                        label=benchmark,\n",
    "                        markeredgecolor=\"white\",\n",
    "                        markeredgewidth=1.5,\n",
    "                    )\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        ax.axhline(y=0, color=\"#BDC3C7\", linestyle=\"--\", alpha=0.8, linewidth=1.5)\n",
    "        ax.set_ylabel(\"Token Savings (%)\")\n",
    "        ax.set_title(f\"{display_name}\", fontsize=12, pad=10)\n",
    "        ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "        ax.tick_params(axis=\"x\", rotation=0)\n",
    "        ax.set_facecolor(\"#FAFAFA\")\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        ncol=1,\n",
    "        fontsize=10,\n",
    "        framealpha=0.9,\n",
    "    )\n",
    "    plt.suptitle(\n",
    "        \"Token Savings by Benchmark\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        y=1.00,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.08, hspace=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb78c9",
   "metadata": {},
   "source": [
    "## Per-Model Bar Chart: Accuracy by Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806951a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = len(filtered_rollout_data)\n",
    "if num_models == 0:\n",
    "    print(\"No data available for this plot.\")\n",
    "else:\n",
    "    methods = [\"Baseline\"] + [f\"t={t}\" for t in thresholds]\n",
    "\n",
    "    fig, axes = plt.subplots(num_models, 1, figsize=(16, 5 * num_models))\n",
    "    if num_models == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for model_idx, (rollout_name, rollout_model_data) in enumerate(\n",
    "        filtered_rollout_data.items()\n",
    "    ):\n",
    "        ax = axes[model_idx]\n",
    "        mapped_name = model_name_map.get(rollout_name, rollout_name)\n",
    "        display_name = short_names.get(mapped_name, rollout_name)\n",
    "\n",
    "        model_benchmarks = []\n",
    "        model_accuracies = {m: [] for m in methods}\n",
    "\n",
    "        for benchmark in benchmarks:\n",
    "            try:\n",
    "                if benchmark not in rollout_model_data:\n",
    "                    continue\n",
    "\n",
    "                model_benchmarks.append(benchmark)\n",
    "                model_accuracies[\"Baseline\"].append(\n",
    "                    rollout_model_data[benchmark][\"accuracy\"]\n",
    "                )\n",
    "\n",
    "                if (\n",
    "                    mapped_name in filtered_thinkbrake_data\n",
    "                    and benchmark in filtered_thinkbrake_data[mapped_name]\n",
    "                ):\n",
    "                    tb_data = filtered_thinkbrake_data[mapped_name][benchmark]\n",
    "                    for thresh in thresholds:\n",
    "                        key = f\"threshold_{thresh}\"\n",
    "                        if key in tb_data:\n",
    "                            model_accuracies[f\"t={thresh}\"].append(\n",
    "                                tb_data[key][\"accuracy\"]\n",
    "                            )\n",
    "                        else:\n",
    "                            model_accuracies[f\"t={thresh}\"].append(0)\n",
    "                else:\n",
    "                    for t in thresholds:\n",
    "                        model_accuracies[f\"t={t}\"].append(0)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if model_benchmarks:\n",
    "            x = np.arange(len(model_benchmarks))\n",
    "            n_methods = len(methods)\n",
    "            width = 0.8 / n_methods\n",
    "\n",
    "            for i, method in enumerate(methods):\n",
    "                try:\n",
    "                    if model_accuracies[method]:\n",
    "                        bars = ax.bar(\n",
    "                            x + (i - n_methods / 2 + 0.5) * width,\n",
    "                            model_accuracies[method],\n",
    "                            width * 0.9,\n",
    "                            label=method,\n",
    "                            color=bar_colors[i % len(bar_colors)],\n",
    "                            edgecolor=\"white\",\n",
    "                            linewidth=0.5,\n",
    "                        )\n",
    "                        # Add percentage labels on bars\n",
    "                        for bar_idx, (bar, val) in enumerate(\n",
    "                            zip(bars, model_accuracies[method])\n",
    "                        ):\n",
    "                            if val > 0:\n",
    "                                baseline_val = model_accuracies[\"Baseline\"][bar_idx]\n",
    "                                if method != \"Baseline\" and val >= baseline_val:\n",
    "                                    text_color = \"#00A94F\"\n",
    "                                    fontweight = \"bold\"\n",
    "                                else:\n",
    "                                    text_color = \"#333333\"\n",
    "                                    fontweight = \"normal\"\n",
    "                                ax.text(\n",
    "                                    bar.get_x() + bar.get_width() / 2,\n",
    "                                    bar.get_height() + 0.5,\n",
    "                                    f\"{val:.1f}%\",\n",
    "                                    ha=\"center\",\n",
    "                                    va=\"bottom\",\n",
    "                                    fontsize=7,\n",
    "                                    color=text_color,\n",
    "                                    fontweight=fontweight,\n",
    "                                    rotation=90,\n",
    "                                )\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            ax.set_ylabel(\"Accuracy (%)\")\n",
    "            ax.set_title(f\"{display_name}\", fontsize=14, pad=10)\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(model_benchmarks, rotation=0, ha=\"center\")\n",
    "            ax.legend(\n",
    "                loc=\"center left\",\n",
    "                bbox_to_anchor=(1.02, 0.5),\n",
    "                fontsize=9,\n",
    "                framealpha=0.9,\n",
    "                ncol=1,\n",
    "                title=\"Method\",\n",
    "            )\n",
    "            ax.grid(True, alpha=0.3, axis=\"y\", linestyle=\"--\")\n",
    "            ax.set_ylim(0, 115)\n",
    "            ax.set_facecolor(\"#FAFAFA\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2069ce20",
   "metadata": {},
   "source": [
    "## Load Detailed Results Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd7582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_detailed_results(model_dirs, benchmarks, best_threshold=\"0.25\"):\n",
    "    all_results = {}\n",
    "\n",
    "    for model_dir in model_dirs:\n",
    "        all_results[model_dir] = {}\n",
    "        for benchmark in benchmarks:\n",
    "            rollout_results = load_detailed_results(model_dir, benchmark, \"rollout\")\n",
    "            thinkbrake_results = load_detailed_results(\n",
    "                model_dir, benchmark, f\"threshold_{best_threshold}\"\n",
    "            )\n",
    "\n",
    "            if rollout_results and thinkbrake_results:\n",
    "                all_results[model_dir][benchmark] = {\n",
    "                    \"rollout\": rollout_results,\n",
    "                    \"thinkbrake\": thinkbrake_results,\n",
    "                }\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8511bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_THRESHOLD = 0.1\n",
    "detailed_results = load_all_detailed_results(\n",
    "    INCLUDE_MODELS, INCLUDE_BENCHMARKS, BEST_THRESHOLD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d1f9e1",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537fb1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_verify import parse, verify\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_multiple_choice_answer(response: str) -> str:\n",
    "    patterns = [\n",
    "        r'[\"\\*]*answer[\"\\*]*\\s*[:=]\\s*[\"\\']?([A-Da-d])[\"\\']?',\n",
    "        r\"(?:the\\s+)?answer\\s+is[:\\s]*([A-Da-d])\\b\",\n",
    "        r\"final\\s+answer[:\\s]*([A-Da-d])\\b\",\n",
    "        r\"(?:choice|option)[:\\s]*([A-Da-d])\\b\",\n",
    "        r\"\\b([A-Da-d])\\s*$\",\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, response, re.IGNORECASE)\n",
    "        if matches:\n",
    "            return matches[-1].upper()\n",
    "\n",
    "    standalone_matches = re.findall(r\"\\b([A-Da-d])\\b\", response)\n",
    "    if standalone_matches:\n",
    "        return standalone_matches[-1].upper()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def verify_multiple_choice(ground_truth: str, predicted: str) -> bool:\n",
    "    \"\"\"Verify if the predicted multiple choice answer matches the ground truth.\"\"\"\n",
    "    if not predicted:\n",
    "        return False\n",
    "    return ground_truth.upper().strip() == predicted.upper().strip()\n",
    "\n",
    "\n",
    "def evaluate_item(item: dict, is_multiple_choice: bool) -> bool:\n",
    "    \"\"\"\n",
    "    Evaluate a single item using the same logic as evaluate.py\n",
    "    \"\"\"\n",
    "    if is_multiple_choice:\n",
    "        ground_truth = item[\"answer\"]\n",
    "        predicted = extract_multiple_choice_answer(item[\"response\"])\n",
    "        return verify_multiple_choice(ground_truth, predicted)\n",
    "    else:\n",
    "        # Math evaluation using math_verify\n",
    "        try:\n",
    "            ground_truth = parse(f\"${item['answer']}$\")\n",
    "            predicted = parse(item[\"response\"])\n",
    "            return verify(ground_truth, predicted)\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "\n",
    "def compute_confusion_matrix(rollout_results, thinkbrake_results, benchmark):\n",
    "    \"\"\"Compute confusion matrix comparing rollout (baseline) vs thinkbrake results\"\"\"\n",
    "    rollout_dict = {r[\"id\"]: r for r in rollout_results}\n",
    "    thinkbrake_dict = {r[\"id\"]: r for r in thinkbrake_results}\n",
    "\n",
    "    # Determine if it's a multiple choice benchmark\n",
    "    general_benchmarks = [\"gpqa-diamond\", \"mmlu-redux\"]\n",
    "    is_multiple_choice = benchmark in general_benchmarks\n",
    "\n",
    "    both_correct = 0\n",
    "    only_rollout_correct = 0\n",
    "    only_thinkbrake_correct = 0\n",
    "    both_wrong = 0\n",
    "\n",
    "    # Find common IDs\n",
    "    common_ids = set(rollout_dict.keys()) & set(thinkbrake_dict.keys())\n",
    "\n",
    "    for pid in common_ids:\n",
    "        rollout_item = rollout_dict[pid]\n",
    "        tb_item = thinkbrake_dict[pid]\n",
    "\n",
    "        # Evaluate using the same method as evaluate.py\n",
    "        rollout_correct = evaluate_item(rollout_item, is_multiple_choice)\n",
    "        tb_correct = evaluate_item(tb_item, is_multiple_choice)\n",
    "\n",
    "        if rollout_correct and tb_correct:\n",
    "            both_correct += 1\n",
    "        elif rollout_correct and not tb_correct:\n",
    "            only_rollout_correct += 1\n",
    "        elif not rollout_correct and tb_correct:\n",
    "            only_thinkbrake_correct += 1\n",
    "        else:\n",
    "            both_wrong += 1\n",
    "\n",
    "    return (\n",
    "        both_correct,\n",
    "        only_rollout_correct,\n",
    "        only_thinkbrake_correct,\n",
    "        both_wrong,\n",
    "        len(common_ids),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6fc21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_data = {}\n",
    "\n",
    "for model_dir, benchmarks_data in detailed_results.items():\n",
    "    display_name = short_names.get(model_name_map.get(model_dir, model_dir), model_dir)\n",
    "    confusion_data[display_name] = {}\n",
    "\n",
    "    for benchmark, data in benchmarks_data.items():\n",
    "        if data[\"rollout\"] and data[\"thinkbrake\"]:\n",
    "            cm = compute_confusion_matrix(\n",
    "                data[\"rollout\"], data[\"thinkbrake\"], benchmark\n",
    "            )\n",
    "            confusion_data[display_name][benchmark] = {\n",
    "                \"both_correct\": cm[0],\n",
    "                \"only_rollout\": cm[1],\n",
    "                \"only_thinkbrake\": cm[2],\n",
    "                \"both_wrong\": cm[3],\n",
    "                \"total\": cm[4],\n",
    "            }\n",
    "\n",
    "\n",
    "n_models = len([m for m in confusion_data if confusion_data[m]])\n",
    "if n_models > 0:\n",
    "    fig, axes = create_dynamic_subplots(n_models, max_cols=3)\n",
    "\n",
    "    for idx, (model_name, bench_data) in enumerate(confusion_data.items()):\n",
    "        if not bench_data:\n",
    "            continue\n",
    "        ax = axes[idx]\n",
    "\n",
    "        total_both_correct = sum(d[\"both_correct\"] for d in bench_data.values())\n",
    "        total_only_rollout = sum(d[\"only_rollout\"] for d in bench_data.values())\n",
    "        total_only_tb = sum(d[\"only_thinkbrake\"] for d in bench_data.values())\n",
    "        total_both_wrong = sum(d[\"both_wrong\"] for d in bench_data.values())\n",
    "        total = sum(d[\"total\"] for d in bench_data.values())\n",
    "\n",
    "        # Calculate accuracies\n",
    "        baseline_acc = (\n",
    "            (total_both_correct + total_only_rollout) / total * 100 if total > 0 else 0\n",
    "        )\n",
    "        tb_acc = (total_both_correct + total_only_tb) / total * 100 if total > 0 else 0\n",
    "\n",
    "        # Create confusion matrix\n",
    "        # Rows: ThinkBrake (Correct/Wrong), Cols: Baseline (Correct/Wrong)\n",
    "        cm = np.array(\n",
    "            [\n",
    "                [\n",
    "                    total_both_correct,\n",
    "                    total_only_tb,\n",
    "                ],  # TB Correct: Both correct, Only TB correct\n",
    "                [\n",
    "                    total_only_rollout,\n",
    "                    total_both_wrong,\n",
    "                ],  # TB Wrong: Only Baseline correct, Both wrong\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Plot heatmap\n",
    "        im = ax.imshow(cm, cmap=\"Blues\")\n",
    "\n",
    "        # Add labels - fixed orientation\n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_xticklabels([\"Baseline ✓\", \"Baseline ✗\"])\n",
    "        ax.set_yticklabels([\"ThinkBrake ✓\", \"ThinkBrake ✗\"])\n",
    "        ax.set_xlabel(\"Baseline (Rollout)\", fontweight=\"bold\")\n",
    "        ax.set_ylabel(f\"ThinkBrake (t={BEST_THRESHOLD})\", fontweight=\"bold\")\n",
    "\n",
    "        # Add text annotations\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                pct = cm[i, j] / total * 100 if total > 0 else 0\n",
    "                text = ax.text(\n",
    "                    j,\n",
    "                    i,\n",
    "                    f\"{cm[i, j]}\\n({pct:.1f}%)\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=11,\n",
    "                    color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n",
    "                )\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"{model_name}\\nBaseline: {baseline_acc:.1f}% | TB: {tb_acc:.1f}%\",\n",
    "            fontsize=11,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Confusion Matrix\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        y=1.02,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd489b",
   "metadata": {},
   "source": [
    "## Confusion Matrix by Benchmark\n",
    "\n",
    "각 벤치마크별로 Baseline과 ThinkBrake의 정답 일치/불일치 패턴을 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc28154",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_dir, benchmarks_data in detailed_results.items():\n",
    "    display_name = short_names.get(model_name_map.get(model_dir, model_dir), model_dir)\n",
    "\n",
    "    available_benchmarks = [b for b in INCLUDE_BENCHMARKS if b in benchmarks_data]\n",
    "    n_benchmarks = len(available_benchmarks)\n",
    "\n",
    "    if n_benchmarks == 0:\n",
    "        continue\n",
    "\n",
    "    fig, axes = create_dynamic_subplots(n_benchmarks, max_cols=3)\n",
    "\n",
    "    for idx, benchmark in enumerate(available_benchmarks):\n",
    "        ax = axes[idx]\n",
    "        data = benchmarks_data[benchmark]\n",
    "\n",
    "        if not data[\"rollout\"] or not data[\"thinkbrake\"]:\n",
    "            continue\n",
    "\n",
    "        cm_result = compute_confusion_matrix(\n",
    "            data[\"rollout\"], data[\"thinkbrake\"], benchmark\n",
    "        )\n",
    "        both_correct, only_rollout, only_tb, both_wrong, total = cm_result\n",
    "\n",
    "        baseline_acc = (both_correct + only_rollout) / total * 100 if total > 0 else 0\n",
    "        tb_acc = (both_correct + only_tb) / total * 100 if total > 0 else 0\n",
    "        diff = tb_acc - baseline_acc\n",
    "\n",
    "        cm = np.array(\n",
    "            [\n",
    "                [both_correct, only_tb],\n",
    "                [only_rollout, both_wrong],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Plot heatmap\n",
    "        im = ax.imshow(cm, cmap=\"Blues\")\n",
    "\n",
    "        # Add labels\n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_xticklabels([\"Base ✓\", \"Base ✗\"], fontsize=9)\n",
    "        ax.set_yticklabels([\"TB ✓\", \"TB ✗\"], fontsize=9)\n",
    "\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                pct = cm[i, j] / total * 100 if total > 0 else 0\n",
    "                ax.text(\n",
    "                    j,\n",
    "                    i,\n",
    "                    f\"{cm[i, j]}\\n({pct:.1f}%)\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=9,\n",
    "                    color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n",
    "                )\n",
    "\n",
    "        diff_color = \"green\" if diff >= 0 else (\"red\" if diff < 0 else \"black\")\n",
    "        diff_sign = \"+\" if diff == 0 else \"\"\n",
    "        ax.set_title(\n",
    "            f\"{benchmark}\\nBase: {baseline_acc:.1f}% → TB: {tb_acc:.1f}% ({diff_sign}{diff:.1f}%)\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "            color=diff_color,\n",
    "        )\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"{display_name} - Confusion Matrix by Benchmark (t={BEST_THRESHOLD})\",\n",
    "        fontsize=13,\n",
    "        fontweight=\"bold\",\n",
    "        y=1.02,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76914b13",
   "metadata": {},
   "source": [
    "## Accuracy vs Token Savings Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94acd6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_THRESHOLDS = [\"0.1\", \"0.25\", \"1.0\", \"2.5\"]\n",
    "\n",
    "\n",
    "def is_pareto_optimal(points):\n",
    "    \"\"\"\n",
    "    Find Pareto optimal points.\n",
    "    Points: list of (accuracy_drop, token_savings) tuples\n",
    "    We want to minimize accuracy_drop and maximize token_savings\n",
    "    \"\"\"\n",
    "    is_optimal = np.ones(len(points), dtype=bool)\n",
    "    for i, (drop_i, save_i) in enumerate(points):\n",
    "        for j, (drop_j, save_j) in enumerate(points):\n",
    "            if i != j:\n",
    "                # j dominates i if j has less drop AND more savings\n",
    "                if drop_j <= drop_i and save_j >= save_i:\n",
    "                    if drop_j < drop_i or save_j > save_i:\n",
    "                        is_optimal[i] = False\n",
    "                        break\n",
    "    return is_optimal\n",
    "\n",
    "\n",
    "# Collect trade-off data\n",
    "tradeoff_data = []\n",
    "\n",
    "for rollout_name, rollout_model_data in filtered_rollout_data.items():\n",
    "    mapped_name = model_name_map.get(rollout_name, rollout_name)\n",
    "    display_name = short_names.get(mapped_name, rollout_name)\n",
    "\n",
    "    if mapped_name not in filtered_thinkbrake_data:\n",
    "        continue\n",
    "\n",
    "    tb_model_data = filtered_thinkbrake_data[mapped_name]\n",
    "\n",
    "    for benchmark in benchmarks:\n",
    "        if benchmark not in rollout_model_data or benchmark not in tb_model_data:\n",
    "            continue\n",
    "\n",
    "        baseline_acc = rollout_model_data[benchmark][\"accuracy\"]\n",
    "        baseline_tokens = rollout_model_data[benchmark][\"avg_token_length\"]\n",
    "\n",
    "        # Add baseline point\n",
    "        tradeoff_data.append(\n",
    "            {\n",
    "                \"model\": display_name,\n",
    "                \"benchmark\": benchmark,\n",
    "                \"threshold\": \"Baseline\",\n",
    "                \"accuracy\": baseline_acc,\n",
    "                \"accuracy_drop\": 0,\n",
    "                \"token_savings\": 0,\n",
    "                \"avg_tokens\": baseline_tokens,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for thresh in ALL_THRESHOLDS:\n",
    "            key = f\"threshold_{thresh}\"\n",
    "            if key in tb_model_data[benchmark]:\n",
    "                tb_acc = tb_model_data[benchmark][key][\"accuracy\"]\n",
    "                tb_tokens = tb_model_data[benchmark][key][\"avg_token_length\"]\n",
    "\n",
    "                acc_drop = baseline_acc - tb_acc\n",
    "                token_savings = (1 - tb_tokens / baseline_tokens) * 100\n",
    "\n",
    "                tradeoff_data.append(\n",
    "                    {\n",
    "                        \"model\": display_name,\n",
    "                        \"benchmark\": benchmark,\n",
    "                        \"threshold\": f\"t={thresh}\",\n",
    "                        \"accuracy\": tb_acc,\n",
    "                        \"accuracy_drop\": acc_drop,\n",
    "                        \"token_savings\": token_savings,\n",
    "                        \"avg_tokens\": tb_tokens,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "df_tradeoff = pd.DataFrame(tradeoff_data)\n",
    "models = df_tradeoff[\"model\"].unique()\n",
    "\n",
    "# Single plot: Pareto Frontier with connected lines\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Background gradient for ideal region\n",
    "ax.fill_between([0, 60], [-12, -12], [0, 0], alpha=0.08, color=\"#27AE60\", zorder=0)\n",
    "ax.axhspan(-12, 0, xmin=0, xmax=1, alpha=0.05, color=\"#27AE60\", zorder=0)\n",
    "\n",
    "# Add subtle grid\n",
    "ax.grid(True, alpha=0.4, linestyle=\"-\", linewidth=0.5, color=\"#E0E0E0\")\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    model_data = df_tradeoff[df_tradeoff[\"model\"] == model]\n",
    "    avg_data = (\n",
    "        model_data.groupby(\"threshold\")\n",
    "        .agg({\"accuracy_drop\": \"mean\", \"token_savings\": \"mean\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Sort by token savings for proper line connection\n",
    "    avg_data = avg_data.sort_values(\"token_savings\")\n",
    "\n",
    "    points = list(zip(avg_data[\"accuracy_drop\"], avg_data[\"token_savings\"]))\n",
    "    pareto_mask = is_pareto_optimal(points)\n",
    "\n",
    "    color = line_colors[idx % len(line_colors)]\n",
    "    marker = markers[idx % len(markers)]\n",
    "\n",
    "    # Draw connecting lines (lighter)\n",
    "    ax.plot(\n",
    "        avg_data[\"token_savings\"],\n",
    "        avg_data[\"accuracy_drop\"],\n",
    "        linewidth=1.5,\n",
    "        color=color,\n",
    "        alpha=0.4,\n",
    "        zorder=2,\n",
    "    )\n",
    "\n",
    "    # Plot non-Pareto points (smaller, faded)\n",
    "    non_pareto = avg_data[~pareto_mask]\n",
    "    ax.scatter(\n",
    "        non_pareto[\"token_savings\"],\n",
    "        non_pareto[\"accuracy_drop\"],\n",
    "        c=color,\n",
    "        s=80,\n",
    "        alpha=0.35,\n",
    "        marker=marker,\n",
    "        edgecolors=\"white\",\n",
    "        linewidth=1,\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "    # Highlight Pareto optimal points (larger, prominent)\n",
    "    pareto_points = avg_data[pareto_mask]\n",
    "    ax.scatter(\n",
    "        pareto_points[\"token_savings\"],\n",
    "        pareto_points[\"accuracy_drop\"],\n",
    "        c=color,\n",
    "        s=200,\n",
    "        marker=marker,\n",
    "        edgecolors=\"#2C3E50\",\n",
    "        linewidth=2.5,\n",
    "        label=model,\n",
    "        zorder=5,\n",
    "    )\n",
    "\n",
    "# Zero line (no accuracy loss)\n",
    "ax.axhline(y=0, color=\"#2C3E50\", linestyle=\"--\", alpha=0.6, linewidth=2, zorder=1)\n",
    "\n",
    "# Ideal region annotation\n",
    "ax.annotate(\n",
    "    \"✓ Ideal Region\\n(High Savings, No Accuracy Loss)\",\n",
    "    xy=(45, -6),\n",
    "    fontsize=11,\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    color=\"#1E8449\",\n",
    "    fontweight=\"bold\",\n",
    "    fontstyle=\"italic\",\n",
    "    bbox=dict(\n",
    "        boxstyle=\"round,pad=0.5\",\n",
    "        facecolor=\"white\",\n",
    "        edgecolor=\"#27AE60\",\n",
    "        alpha=0.9,\n",
    "        linewidth=1.5,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Direction arrow with better styling\n",
    "ax.annotate(\n",
    "    \"\",\n",
    "    xy=(52, -9),\n",
    "    xytext=(15, 4),\n",
    "    arrowprops=dict(\n",
    "        arrowstyle=\"-|>\", color=\"#1E8449\", lw=2.5, connectionstyle=\"arc3,rad=-0.1\"\n",
    "    ),\n",
    ")\n",
    "ax.text(\n",
    "    36,\n",
    "    -4,\n",
    "    \"Better\",\n",
    "    fontsize=11,\n",
    "    color=\"#1E8449\",\n",
    "    fontweight=\"bold\",\n",
    "    rotation=-18,\n",
    "    fontstyle=\"italic\",\n",
    ")\n",
    "\n",
    "# Axis styling\n",
    "ax.set_xlabel(\"Token Savings (%)\", fontsize=13, fontweight=\"bold\", labelpad=10)\n",
    "ax.set_ylabel(\"Accuracy Change (%)\", fontsize=13, fontweight=\"bold\", labelpad=10)\n",
    "ax.set_title(\n",
    "    \"Accuracy vs Token Savings Trade-off (Pareto Frontier)\",\n",
    "    fontsize=15,\n",
    "    fontweight=\"bold\",\n",
    "    pad=15,\n",
    ")\n",
    "\n",
    "# Legend with better positioning\n",
    "legend = ax.legend(\n",
    "    loc=\"upper left\",\n",
    "    fontsize=10,\n",
    "    framealpha=0.95,\n",
    "    edgecolor=\"#CCCCCC\",\n",
    "    fancybox=True,\n",
    "    shadow=True,\n",
    "    title=\"Model\",\n",
    "    title_fontsize=11,\n",
    ")\n",
    "legend.get_title().set_fontweight(\"bold\")\n",
    "\n",
    "# Set axis limits with padding\n",
    "ax.set_xlim(-2, 58)\n",
    "ax.set_ylim(-12, 8)\n",
    "\n",
    "# Background color\n",
    "ax.set_facecolor(\"#FAFAFA\")\n",
    "\n",
    "# Spine styling\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(\"#CCCCCC\")\n",
    "    spine.set_linewidth(1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c584058",
   "metadata": {},
   "source": [
    "## Token Length Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770886af",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_models = df_tokens[\"model\"].unique()\n",
    "\n",
    "for model in available_models:\n",
    "    model_data = df_tokens[df_tokens[\"model\"] == model]\n",
    "    benchmarks_in_model = model_data[\"benchmark\"].unique()\n",
    "    n_benchmarks = len(benchmarks_in_model)\n",
    "\n",
    "    if n_benchmarks == 0:\n",
    "        continue\n",
    "\n",
    "    fig, axes = create_dynamic_subplots(n_benchmarks, max_cols=3)\n",
    "\n",
    "    for idx, benchmark in enumerate(benchmarks_in_model):\n",
    "        ax = axes[idx]\n",
    "        bench_data = model_data[model_data[\"benchmark\"] == benchmark]\n",
    "\n",
    "        # Box plot by correctness\n",
    "        baseline_correct = bench_data[\n",
    "            (bench_data[\"method\"] == \"Baseline\") & (bench_data[\"correct\"])\n",
    "        ][\"tokens\"]\n",
    "        baseline_wrong = bench_data[\n",
    "            (bench_data[\"method\"] == \"Baseline\") & (~bench_data[\"correct\"])\n",
    "        ][\"tokens\"]\n",
    "        tb_correct = bench_data[\n",
    "            (bench_data[\"method\"] == f\"ThinkBrake (t={BEST_THRESHOLD})\")\n",
    "            & (bench_data[\"correct\"])\n",
    "        ][\"tokens\"]\n",
    "        tb_wrong = bench_data[\n",
    "            (bench_data[\"method\"] == f\"ThinkBrake (t={BEST_THRESHOLD})\")\n",
    "            & (~bench_data[\"correct\"])\n",
    "        ][\"tokens\"]\n",
    "\n",
    "        data_to_plot = [baseline_correct, baseline_wrong, tb_correct, tb_wrong]\n",
    "        positions = [0, 1, 3, 4]\n",
    "        colors_box = [\"#27AE60\", \"#E74C3C\", \"#27AE60\", \"#E74C3C\"]\n",
    "\n",
    "        # showfliers=False to hide outliers\n",
    "        bp = ax.boxplot(\n",
    "            data_to_plot,\n",
    "            positions=positions,\n",
    "            patch_artist=True,\n",
    "            widths=0.6,\n",
    "            showfliers=False,\n",
    "        )\n",
    "\n",
    "        for patch, color in zip(bp[\"boxes\"], colors_box):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.6)\n",
    "\n",
    "        ax.set_xticks([0.5, 3.5])\n",
    "        ax.set_xticklabels([\"Baseline\", \"ThinkBrake\"])\n",
    "        ax.set_ylabel(\"Token Count\")\n",
    "\n",
    "        # Add reduction stats\n",
    "        baseline_mean = bench_data[bench_data[\"method\"] == \"Baseline\"][\"tokens\"].mean()\n",
    "        tb_mean = bench_data[\n",
    "            bench_data[\"method\"] == f\"ThinkBrake (t={BEST_THRESHOLD})\"\n",
    "        ][\"tokens\"].mean()\n",
    "        reduction = (1 - tb_mean / baseline_mean) * 100 if baseline_mean > 0 else 0\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"{benchmark}\\n(Reduction: {reduction:.1f}%)\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "            color=\"green\" if reduction > 0 else \"red\",\n",
    "        )\n",
    "        ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "        ax.set_facecolor(\"#FAFAFA\")\n",
    "\n",
    "        # Legend\n",
    "        legend_elements = [\n",
    "            Patch(facecolor=\"#27AE60\", alpha=0.6, label=\"Correct\"),\n",
    "            Patch(facecolor=\"#E74C3C\", alpha=0.6, label=\"Wrong\"),\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc=\"upper right\", fontsize=7)\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"Token Length by Correctness - {model}\", fontsize=14, fontweight=\"bold\", y=1.02\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c854672e",
   "metadata": {},
   "source": [
    "## Per-Sample Analysis: Token Change vs Correctness Change\n",
    "\n",
    "개별 샘플 수준에서 토큰 변화와 정답 변화의 관계를 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6e6e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_analysis = []\n",
    "\n",
    "for model_dir, benchmarks_data in detailed_results.items():\n",
    "    display_name = short_names.get(model_name_map.get(model_dir, model_dir), model_dir)\n",
    "\n",
    "    for benchmark, data in benchmarks_data.items():\n",
    "        if not data[\"rollout\"] or not data[\"thinkbrake\"]:\n",
    "            continue\n",
    "\n",
    "        is_mc = benchmark in [\"gpqa-diamond\", \"mmlu-redux\"]\n",
    "\n",
    "        rollout_dict = {r[\"id\"]: r for r in data[\"rollout\"]}\n",
    "        tb_dict = {r[\"id\"]: r for r in data[\"thinkbrake\"]}\n",
    "\n",
    "        common_ids = set(rollout_dict.keys()) & set(tb_dict.keys())\n",
    "\n",
    "        for pid in common_ids:\n",
    "            r_item = rollout_dict[pid]\n",
    "            tb_item = tb_dict[pid]\n",
    "\n",
    "            r_tokens = r_item[\"token_length\"]\n",
    "            tb_tokens = tb_item[\"token_length\"]\n",
    "\n",
    "            r_correct = evaluate_item(r_item, is_mc)\n",
    "            tb_correct = evaluate_item(tb_item, is_mc)\n",
    "\n",
    "            token_change = tb_tokens - r_tokens\n",
    "            token_reduction_pct = (\n",
    "                ((r_tokens - tb_tokens) / r_tokens * 100) if r_tokens > 0 else 0\n",
    "            )\n",
    "\n",
    "            if r_correct and tb_correct:\n",
    "                outcome = \"Both Correct\"\n",
    "            elif r_correct and not tb_correct:\n",
    "                outcome = \"Lost (was correct)\"\n",
    "            elif not r_correct and tb_correct:\n",
    "                outcome = \"Gained (was wrong)\"\n",
    "            else:\n",
    "                outcome = \"Both Wrong\"\n",
    "\n",
    "            sample_analysis.append(\n",
    "                {\n",
    "                    \"model\": display_name,\n",
    "                    \"benchmark\": benchmark,\n",
    "                    \"id\": pid,\n",
    "                    \"baseline_tokens\": r_tokens,\n",
    "                    \"thinkbrake_tokens\": tb_tokens,\n",
    "                    \"token_change\": token_change,\n",
    "                    \"token_reduction_pct\": token_reduction_pct,\n",
    "                    \"baseline_correct\": r_correct,\n",
    "                    \"thinkbrake_correct\": tb_correct,\n",
    "                    \"outcome\": outcome,\n",
    "                }\n",
    "            )\n",
    "\n",
    "df_samples = pd.DataFrame(sample_analysis)\n",
    "\n",
    "lower_pct, upper_pct = 1, 99\n",
    "x_lower, x_upper = np.percentile(df_samples[\"baseline_tokens\"], [lower_pct, upper_pct])\n",
    "y_lower, y_upper = np.percentile(\n",
    "    df_samples[\"token_reduction_pct\"], [lower_pct, upper_pct]\n",
    ")\n",
    "\n",
    "df_filtered = df_samples[\n",
    "    (df_samples[\"baseline_tokens\"] >= x_lower)\n",
    "    & (df_samples[\"baseline_tokens\"] <= x_upper)\n",
    "    & (df_samples[\"token_reduction_pct\"] >= y_lower)\n",
    "    & (df_samples[\"token_reduction_pct\"] <= y_upper)\n",
    "].copy()\n",
    "\n",
    "print(\n",
    "    f\"Original samples: {len(df_samples):,}, After filtering: {len(df_filtered):,} ({len(df_filtered)/len(df_samples)*100:.1f}%)\"\n",
    ")\n",
    "\n",
    "outcomes = [\"Both Correct\", \"Both Wrong\", \"Gained (was wrong)\", \"Lost (was correct)\"]\n",
    "outcome_colors = [\"#27AE60\", \"#95A5A6\", \"#3498DB\", \"#E74C3C\"]\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "gs = fig.add_gridspec(\n",
    "    2, 2, width_ratios=[4, 1], height_ratios=[1, 4], hspace=0.05, wspace=0.05\n",
    ")\n",
    "\n",
    "ax_main = fig.add_subplot(gs[1, 0])\n",
    "ax_histx = fig.add_subplot(gs[0, 0], sharex=ax_main)\n",
    "ax_histy = fig.add_subplot(gs[1, 1], sharey=ax_main)\n",
    "\n",
    "hb = ax_main.hexbin(\n",
    "    df_filtered[\"baseline_tokens\"],\n",
    "    df_filtered[\"token_reduction_pct\"],\n",
    "    gridsize=30,\n",
    "    cmap=\"Greys\",\n",
    "    alpha=0.3,\n",
    "    mincnt=1,\n",
    ")\n",
    "\n",
    "for i, outcome in enumerate(outcomes):\n",
    "    subset = df_filtered[df_filtered[\"outcome\"] == outcome]\n",
    "    n_samples = len(subset)\n",
    "\n",
    "    # Adjust marker size based on category (larger for rare categories)\n",
    "    if outcome in [\"Gained (was wrong)\", \"Lost (was correct)\"]:\n",
    "        size = 60\n",
    "        alpha = 0.8\n",
    "        zorder = 5\n",
    "    else:\n",
    "        size = 25\n",
    "        alpha = 0.4\n",
    "        zorder = 3\n",
    "\n",
    "    ax_main.scatter(\n",
    "        subset[\"baseline_tokens\"],\n",
    "        subset[\"token_reduction_pct\"],\n",
    "        c=outcome_colors[i],\n",
    "        label=f\"{outcome} (n={n_samples})\",\n",
    "        alpha=alpha,\n",
    "        s=size,\n",
    "        edgecolors=(\n",
    "            \"white\"\n",
    "            if outcome in [\"Gained (was wrong)\", \"Lost (was correct)\"]\n",
    "            else \"none\"\n",
    "        ),\n",
    "        linewidth=0.5,\n",
    "        zorder=zorder,\n",
    "    )\n",
    "\n",
    "ax_main.axhline(\n",
    "    y=0, color=\"#2C3E50\", linestyle=\"--\", alpha=0.7, linewidth=2, label=\"No change\"\n",
    ")\n",
    "median_val = df_filtered[\"token_reduction_pct\"].median()\n",
    "ax_main.axhline(\n",
    "    y=median_val,\n",
    "    color=\"#9B59B6\",\n",
    "    linestyle=\":\",\n",
    "    alpha=0.6,\n",
    "    linewidth=1.5,\n",
    "    label=f\"Median ({median_val:.1f}%)\",\n",
    ")\n",
    "\n",
    "ax_main.set_xlabel(\"Baseline Token Count\", fontsize=12, fontweight=\"bold\")\n",
    "ax_main.set_ylabel(\"Token Reduction (%)\", fontsize=12, fontweight=\"bold\")\n",
    "ax_main.legend(loc=\"lower right\", fontsize=9, framealpha=0.95)\n",
    "ax_main.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "ax_main.set_facecolor(\"#FAFAFA\")\n",
    "\n",
    "# Marginal histogram for x-axis (baseline tokens)\n",
    "for i, outcome in enumerate(outcomes):\n",
    "    subset = df_filtered[df_filtered[\"outcome\"] == outcome]\n",
    "    ax_histx.hist(\n",
    "        subset[\"baseline_tokens\"],\n",
    "        bins=40,\n",
    "        alpha=0.5,\n",
    "        color=outcome_colors[i],\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=0.3,\n",
    "    )\n",
    "ax_histx.set_ylabel(\"Count\", fontsize=10)\n",
    "ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "ax_histx.set_facecolor(\"#FAFAFA\")\n",
    "ax_histx.grid(True, alpha=0.3, axis=\"y\", linestyle=\"--\")\n",
    "\n",
    "for i, outcome in enumerate(outcomes):\n",
    "    subset = df_filtered[df_filtered[\"outcome\"] == outcome]\n",
    "    ax_histy.hist(\n",
    "        subset[\"token_reduction_pct\"],\n",
    "        bins=40,\n",
    "        alpha=0.5,\n",
    "        color=outcome_colors[i],\n",
    "        orientation=\"horizontal\",\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=0.3,\n",
    "    )\n",
    "ax_histy.set_xlabel(\"Count\", fontsize=10)\n",
    "ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "ax_histy.set_facecolor(\"#FAFAFA\")\n",
    "ax_histy.grid(True, alpha=0.3, axis=\"x\", linestyle=\"--\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Token Reduction vs Baseline Length by Outcome (1-99 percentile)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    y=0.98,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42cde51",
   "metadata": {},
   "source": [
    "## Threshold Selection Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a1e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_knee_point(x, y):\n",
    "    if len(x) < 3:\n",
    "        return None, None\n",
    "\n",
    "    x_norm = (x - x.min()) / (x.max() - x.min() + 1e-10)\n",
    "    y_norm = (y - y.min()) / (y.max() - y.min() + 1e-10)\n",
    "\n",
    "    line_vec = np.array([x_norm[-1] - x_norm[0], y_norm[-1] - y_norm[0]])\n",
    "    line_vec_norm = line_vec / (np.linalg.norm(line_vec) + 1e-10)\n",
    "\n",
    "    distances = []\n",
    "    for i in range(len(x)):\n",
    "        point_vec = np.array([x_norm[i] - x_norm[0], y_norm[i] - y_norm[0]])\n",
    "        proj_length = np.dot(point_vec, line_vec_norm)\n",
    "        proj = proj_length * line_vec_norm\n",
    "        perp_vec = point_vec - proj\n",
    "        distances.append(np.linalg.norm(perp_vec))\n",
    "\n",
    "    knee_idx = np.argmax(distances)\n",
    "    return x[knee_idx], y[knee_idx], knee_idx\n",
    "\n",
    "\n",
    "knee_analysis_data = []\n",
    "all_thresholds_numeric = [0.0] + [float(t) for t in ALL_THRESHOLDS]\n",
    "\n",
    "for rollout_name, rollout_model_data in filtered_rollout_data.items():\n",
    "    mapped_name = model_name_map.get(rollout_name, rollout_name)\n",
    "    display_name = short_names.get(mapped_name, rollout_name)\n",
    "\n",
    "    if mapped_name not in filtered_thinkbrake_data:\n",
    "        continue\n",
    "\n",
    "    tb_model_data = filtered_thinkbrake_data[mapped_name]\n",
    "\n",
    "    model_acc_drops = {t: [] for t in all_thresholds_numeric}\n",
    "    model_token_savings = {t: [] for t in all_thresholds_numeric}\n",
    "\n",
    "    for benchmark in benchmarks:\n",
    "        if benchmark not in rollout_model_data or benchmark not in tb_model_data:\n",
    "            continue\n",
    "\n",
    "        baseline_acc = rollout_model_data[benchmark][\"accuracy\"]\n",
    "        baseline_tokens = rollout_model_data[benchmark][\"avg_token_length\"]\n",
    "\n",
    "        # Baseline point\n",
    "        model_acc_drops[0.0].append(0)\n",
    "        model_token_savings[0.0].append(0)\n",
    "\n",
    "        for thresh in ALL_THRESHOLDS:\n",
    "            key = f\"threshold_{thresh}\"\n",
    "            if key in tb_model_data[benchmark]:\n",
    "                tb_acc = tb_model_data[benchmark][key][\"accuracy\"]\n",
    "                tb_tokens = tb_model_data[benchmark][key][\"avg_token_length\"]\n",
    "\n",
    "                model_acc_drops[float(thresh)].append(baseline_acc - tb_acc)\n",
    "                model_token_savings[float(thresh)].append(\n",
    "                    (1 - tb_tokens / baseline_tokens) * 100\n",
    "                )\n",
    "\n",
    "    for t in all_thresholds_numeric:\n",
    "        if model_acc_drops[t]:\n",
    "            knee_analysis_data.append(\n",
    "                {\n",
    "                    \"model\": display_name,\n",
    "                    \"threshold\": t,\n",
    "                    \"avg_acc_drop\": np.mean(model_acc_drops[t]),\n",
    "                    \"avg_token_savings\": np.mean(model_token_savings[t]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "df_knee = pd.DataFrame(knee_analysis_data)\n",
    "\n",
    "n_models = len(df_knee[\"model\"].unique())\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "knee_results = {}\n",
    "\n",
    "for idx, model in enumerate(df_knee[\"model\"].unique()):\n",
    "    ax = axes[idx]\n",
    "    model_data = df_knee[df_knee[\"model\"] == model].sort_values(\"threshold\")\n",
    "\n",
    "    x = model_data[\"avg_token_savings\"].values\n",
    "    y = model_data[\"avg_acc_drop\"].values\n",
    "    thresholds = model_data[\"threshold\"].values\n",
    "\n",
    "    ax.plot(\n",
    "        x,\n",
    "        y,\n",
    "        \"o-\",\n",
    "        linewidth=2,\n",
    "        markersize=10,\n",
    "        color=\"#3498DB\",\n",
    "        markeredgecolor=\"white\",\n",
    "        markeredgewidth=2,\n",
    "        label=\"Trade-off curve\",\n",
    "    )\n",
    "\n",
    "    if len(x) >= 3:\n",
    "        knee_x, knee_y, knee_idx = find_knee_point(x, y)\n",
    "        knee_thresh = thresholds[knee_idx]\n",
    "        knee_results[model] = {\n",
    "            \"threshold\": knee_thresh,\n",
    "            \"token_savings\": knee_x,\n",
    "            \"acc_drop\": knee_y,\n",
    "        }\n",
    "\n",
    "        ax.scatter(\n",
    "            [knee_x],\n",
    "            [knee_y],\n",
    "            s=300,\n",
    "            c=\"#E74C3C\",\n",
    "            marker=\"*\",\n",
    "            zorder=5,\n",
    "            edgecolors=\"white\",\n",
    "            linewidth=2,\n",
    "            label=f\"Knee (t={knee_thresh})\",\n",
    "        )\n",
    "\n",
    "        ax.plot(\n",
    "            [x[0], x[-1]], [y[0], y[-1]], \"--\", color=\"gray\", alpha=0.5, linewidth=1.5\n",
    "        )\n",
    "\n",
    "        # Annotate threshold values\n",
    "        for i, (xi, yi, ti) in enumerate(zip(x, y, thresholds)):\n",
    "            offset = (5, 5) if i % 2 == 0 else (-15, -15)\n",
    "            ax.annotate(\n",
    "                f\"t={ti}\",\n",
    "                (xi, yi),\n",
    "                textcoords=\"offset points\",\n",
    "                xytext=offset,\n",
    "                fontsize=8,\n",
    "                alpha=0.7,\n",
    "            )\n",
    "\n",
    "    ax.axhline(y=0, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=1.5)\n",
    "    ax.set_xlabel(\"Token Savings (%)\")\n",
    "    ax.set_ylabel(\"Accuracy Drop (%)\")\n",
    "    ax.set_title(f\"{model}\", fontsize=11, fontweight=\"bold\")\n",
    "    ax.legend(loc=\"upper left\", fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_facecolor(\"#FAFAFA\")\n",
    "\n",
    "for idx in range(n_models, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Optimal Trade-off Threshold per Model\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
