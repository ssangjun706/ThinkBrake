{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"axes.titlesize\"] = 16\n",
    "plt.rcParams[\"axes.labelsize\"] = 14\n",
    "\n",
    "model_colors = [\n",
    "    \"#1f77b4\",\n",
    "    \"#ff7f0e\",\n",
    "    \"#2ca02c\",\n",
    "    \"#d62728\",\n",
    "    \"#9467bd\",\n",
    "    \"#8c564b\",\n",
    "    \"#e377c2\",\n",
    "    \"#7f7f7f\",\n",
    "]\n",
    "benchmark_colors = [\n",
    "    \"#1f77b4\",\n",
    "    \"#ff7f0e\",\n",
    "    \"#2ca02c\",\n",
    "    \"#d62728\",\n",
    "    \"#9467bd\",\n",
    "    \"#8c564b\",\n",
    "    \"#e377c2\",\n",
    "    \"#7f7f7f\",\n",
    "    \"#bcbd22\",\n",
    "    \"#17becf\",\n",
    "]\n",
    "bar_colors = [\n",
    "    \"#4a4a4a\",\n",
    "    \"#4c72b0\",\n",
    "    \"#dd8452\",\n",
    "    \"#55a868\",\n",
    "    \"#c44e52\",\n",
    "    \"#8172b3\",\n",
    "    \"#937860\",\n",
    "    \"#da8bc3\",\n",
    "]\n",
    "bar_threshold_colors = [\n",
    "    \"#4c72b0\",\n",
    "    \"#dd8452\",\n",
    "    \"#55a868\",\n",
    "    \"#c44e52\",\n",
    "    \"#8172b3\",\n",
    "    \"#937860\",\n",
    "    \"#da8bc3\",\n",
    "    \"#64b5cd\",\n",
    "]\n",
    "markers = [\"o\", \"s\", \"^\", \"D\", \"v\", \"p\", \"h\", \"*\"]\n",
    "\n",
    "\n",
    "def calc_subplot_layout(n_items, max_cols=4):\n",
    "    \"\"\"동적으로 subplot 레이아웃 계산 (rows, cols, figsize)\"\"\"\n",
    "    if n_items == 0:\n",
    "        return 1, 1, (6, 4)\n",
    "    cols = min(n_items, max_cols)\n",
    "    rows = math.ceil(n_items / cols)\n",
    "    fig_width = 5 * cols\n",
    "    fig_height = 4.5 * rows\n",
    "    return rows, cols, (fig_width, fig_height)\n",
    "\n",
    "\n",
    "def create_dynamic_subplots(n_items, max_cols=4, height_per_row=4.5, width_per_col=5):\n",
    "    \"\"\"동적으로 subplot 생성\"\"\"\n",
    "    rows, cols, figsize = calc_subplot_layout(n_items, max_cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "\n",
    "    # 항상 1D 배열로 반환\n",
    "    if n_items == 1:\n",
    "        axes = np.array([axes])\n",
    "    else:\n",
    "        axes = np.array(axes).flatten()\n",
    "\n",
    "    # 사용하지 않는 subplot 숨기기\n",
    "    for i in range(n_items, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "# Load data\n",
    "with open(\"../outputs/leaderboard_rollout.json\", \"r\") as f:\n",
    "    rollout_data = json.load(f)\n",
    "\n",
    "with open(\"../outputs/leaderboard_thinkbrake.json\", \"r\") as f:\n",
    "    thinkbrake_data = json.load(f)\n",
    "\n",
    "\n",
    "INCLUDE_MODELS = [\n",
    "    \"Qwen_Qwen3-4B-Thinking-2507\",\n",
    "    \"Qwen_Qwen3-4B\",\n",
    "    \"Qwen_Qwen3-14B\",\n",
    "    \"deepseek-ai_DeepSeek-R1-Distill-Llama-8B\",\n",
    "    \"deepseek-ai_DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    # 'openai_gpt-oss-20b',\n",
    "]\n",
    "\n",
    "INCLUDE_BENCHMARKS = [\n",
    "    \"gsm8k\",\n",
    "    \"math500\",\n",
    "    \"aime2024\",\n",
    "    \"aime2025\",\n",
    "    \"gpqa-diamond\",\n",
    "    \"mmlu-redux\",\n",
    "]\n",
    "\n",
    "INCLUDE_THRESHOLDS = [\n",
    "    # '0.0',\n",
    "    \"0.25\",\n",
    "    \"1.0\",\n",
    "    \"2.5\",\n",
    "    \"5.0\",\n",
    "]\n",
    "\n",
    "model_name_map = {\n",
    "    \"Qwen_Qwen3-4B-Thinking-2507\": \"Qwen/Qwen3-4B-Thinking-2507\",\n",
    "    \"Qwen_Qwen3-4B\": \"Qwen/Qwen3-4B\",\n",
    "    \"Qwen_Qwen3-14B\": \"Qwen/Qwen3-14B\",\n",
    "    \"deepseek-ai_DeepSeek-R1-Distill-Llama-8B\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    \"deepseek-ai_DeepSeek-R1-Distill-Qwen-7B\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    \"openai_gpt-oss-20b\": \"openai/gpt-oss-20b\",\n",
    "}\n",
    "\n",
    "short_names = {\n",
    "    \"Qwen/Qwen3-4B-Thinking-2507\": \"Qwen3-4B-2507\",\n",
    "    \"Qwen/Qwen3-4B\": \"Qwen3-4B\",\n",
    "    \"Qwen/Qwen3-14B\": \"Qwen3-14B\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\": \"DS-R1-8B\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\": \"DS-R1-7B\",\n",
    "    \"openai/gpt-oss-20b\": \"GPT-OSS-20B\",\n",
    "}\n",
    "\n",
    "benchmarks = INCLUDE_BENCHMARKS\n",
    "# Sort thresholds numerically (ascending order)\n",
    "thresholds = sorted(INCLUDE_THRESHOLDS, key=lambda x: float(x))\n",
    "\n",
    "# 실제로 데이터가 있는 모델만 필터링\n",
    "filtered_rollout_data = {k: v for k, v in rollout_data.items() if k in INCLUDE_MODELS}\n",
    "filtered_thinkbrake_data = {\n",
    "    k: v\n",
    "    for k, v in thinkbrake_data.items()\n",
    "    if k in [model_name_map.get(m, m) for m in INCLUDE_MODELS]\n",
    "}\n",
    "baseline_data = filtered_rollout_data\n",
    "\n",
    "print(\"✅ Data loaded and filtered successfully!\")\n",
    "print(f\"Included models (rollout): {list(filtered_rollout_data.keys())}\")\n",
    "print(f\"Included models (thinkbrake): {list(filtered_thinkbrake_data.keys())}\")\n",
    "print(f\"Included benchmarks: {benchmarks}\")\n",
    "print(f\"Included thresholds: {thresholds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf6f6a",
   "metadata": {},
   "source": [
    "## 1. Accuracy by Threshold (per model, per benchmark)\n",
    "Visualize how accuracy changes as threshold increases for each model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6707bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동적 레이아웃으로 subplot 생성\n",
    "n_benchmarks = len(benchmarks)\n",
    "fig, axes = create_dynamic_subplots(n_benchmarks, max_cols=4)\n",
    "\n",
    "for idx, benchmark in enumerate(benchmarks):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    for model_idx, (model_name, model_data) in enumerate(\n",
    "        filtered_thinkbrake_data.items()\n",
    "    ):\n",
    "        try:\n",
    "            if benchmark not in model_data:\n",
    "                continue\n",
    "\n",
    "            bench_data = model_data[benchmark]\n",
    "            x_vals = []\n",
    "            y_vals = []\n",
    "\n",
    "            for thresh in thresholds:\n",
    "                key = f\"threshold_{thresh}\"\n",
    "                if key in bench_data:\n",
    "                    x_vals.append(float(thresh))\n",
    "                    y_vals.append(bench_data[key][\"accuracy\"])\n",
    "\n",
    "            if x_vals:\n",
    "                # ThinkBrake line (solid)\n",
    "                ax.plot(\n",
    "                    x_vals,\n",
    "                    y_vals,\n",
    "                    marker=markers[model_idx % len(markers)],\n",
    "                    color=model_colors[model_idx % len(model_colors)],\n",
    "                    linewidth=2,\n",
    "                    markersize=8,\n",
    "                    label=short_names.get(model_name, model_name),\n",
    "                )\n",
    "\n",
    "                # Add baseline horizontal line (dashed)\n",
    "                # Find corresponding rollout name\n",
    "                for rollout_name in filtered_rollout_data:\n",
    "                    if model_name_map.get(rollout_name) == model_name:\n",
    "                        if benchmark in filtered_rollout_data[rollout_name]:\n",
    "                            baseline_acc = filtered_rollout_data[rollout_name][\n",
    "                                benchmark\n",
    "                            ][\"accuracy\"]\n",
    "                            ax.axhline(\n",
    "                                y=baseline_acc,\n",
    "                                color=model_colors[model_idx % len(model_colors)],\n",
    "                                linestyle=\"--\",\n",
    "                                alpha=0.5,\n",
    "                                linewidth=1.5,\n",
    "                            )\n",
    "                        break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    ax.set_xlabel(\"Threshold\")\n",
    "    ax.set_ylabel(\"Accuracy (%)\")\n",
    "    ax.set_title(f\"{benchmark}\")\n",
    "    ax.legend(loc=\"best\", fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Accuracy by Threshold\", fontsize=16, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f3fd39",
   "metadata": {},
   "source": [
    "## 2. Token Savings by Threshold\n",
    "Observe how token savings (%) increases as threshold increases (higher = more efficient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd2db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동적 레이아웃으로 subplot 생성\n",
    "n_benchmarks = len(benchmarks)\n",
    "fig, axes = create_dynamic_subplots(n_benchmarks, max_cols=3)\n",
    "\n",
    "for idx, benchmark in enumerate(benchmarks):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    for model_idx, (model_name, model_data) in enumerate(\n",
    "        filtered_thinkbrake_data.items()\n",
    "    ):\n",
    "        try:\n",
    "            if benchmark not in model_data:\n",
    "                continue\n",
    "\n",
    "            # Find baseline token length for this model/benchmark\n",
    "            baseline_tokens = None\n",
    "            for rollout_name in filtered_rollout_data:\n",
    "                if model_name_map.get(rollout_name) == model_name:\n",
    "                    if benchmark in filtered_rollout_data[rollout_name]:\n",
    "                        baseline_tokens = filtered_rollout_data[rollout_name][\n",
    "                            benchmark\n",
    "                        ][\"avg_token_length\"]\n",
    "                    break\n",
    "\n",
    "            if baseline_tokens is None:\n",
    "                continue\n",
    "\n",
    "            bench_data = model_data[benchmark]\n",
    "            x_vals = []\n",
    "            y_vals = []\n",
    "\n",
    "            for thresh in thresholds:\n",
    "                key = f\"threshold_{thresh}\"\n",
    "                if key in bench_data:\n",
    "                    x_vals.append(float(thresh))\n",
    "                    # Calculate token savings % (higher = more efficient)\n",
    "                    token_savings = (\n",
    "                        1 - bench_data[key][\"avg_token_length\"] / baseline_tokens\n",
    "                    ) * 100\n",
    "                    y_vals.append(token_savings)\n",
    "\n",
    "            if x_vals:\n",
    "                # ThinkBrake line (solid)\n",
    "                ax.plot(\n",
    "                    x_vals,\n",
    "                    y_vals,\n",
    "                    marker=markers[model_idx % len(markers)],\n",
    "                    color=model_colors[model_idx % len(model_colors)],\n",
    "                    linewidth=2,\n",
    "                    markersize=8,\n",
    "                    label=short_names.get(model_name, model_name),\n",
    "                )\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    ax.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5, linewidth=1)\n",
    "    ax.set_xlabel(\"Threshold\")\n",
    "    ax.set_ylabel(\"Token Savings (%)\")\n",
    "    ax.set_title(f\"{benchmark}\")\n",
    "    ax.legend(loc=\"best\", fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Token Savings by Threshold (vs Baseline)\", fontsize=16, fontweight=\"bold\", y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8530d17",
   "metadata": {},
   "source": [
    "## 7. Average Performance by Model (Bar Chart)\n",
    "Compare average accuracy across all benchmarks for each model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b6a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average accuracy by model and threshold\n",
    "avg_data = []\n",
    "\n",
    "for rollout_name, rollout_model_data in filtered_rollout_data.items():\n",
    "    try:\n",
    "        mapped_name = model_name_map.get(rollout_name, rollout_name)\n",
    "        display_name = short_names.get(mapped_name, rollout_name)\n",
    "\n",
    "        # Baseline average\n",
    "        baseline_accs = []\n",
    "        for b in benchmarks:\n",
    "            try:\n",
    "                if b in rollout_model_data:\n",
    "                    baseline_accs.append(rollout_model_data[b][\"accuracy\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if baseline_accs:\n",
    "            avg_data.append(\n",
    "                {\n",
    "                    \"model\": display_name,\n",
    "                    \"type\": \"Baseline\",\n",
    "                    \"avg_accuracy\": np.mean(baseline_accs),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # ThinkBrake average for each threshold\n",
    "        if mapped_name in filtered_thinkbrake_data:\n",
    "            for thresh in thresholds:\n",
    "                tb_accs = []\n",
    "                for benchmark in benchmarks:\n",
    "                    try:\n",
    "                        if benchmark in filtered_thinkbrake_data[mapped_name]:\n",
    "                            key = f\"threshold_{thresh}\"\n",
    "                            if key in filtered_thinkbrake_data[mapped_name][benchmark]:\n",
    "                                tb_accs.append(\n",
    "                                    filtered_thinkbrake_data[mapped_name][benchmark][\n",
    "                                        key\n",
    "                                    ][\"accuracy\"]\n",
    "                                )\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if tb_accs:\n",
    "                    avg_data.append(\n",
    "                        {\n",
    "                            \"model\": display_name,\n",
    "                            \"type\": f\"t={thresh}\",\n",
    "                            \"avg_accuracy\": np.mean(tb_accs),\n",
    "                        }\n",
    "                    )\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if avg_data:\n",
    "    df_avg = pd.DataFrame(avg_data)\n",
    "\n",
    "    # Grouped bar chart\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    models = df_avg[\"model\"].unique()\n",
    "    types = df_avg[\"type\"].unique()\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.12\n",
    "\n",
    "    for i, t in enumerate(types):\n",
    "        try:\n",
    "            type_data = df_avg[df_avg[\"type\"] == t]\n",
    "            values = [\n",
    "                (\n",
    "                    type_data[type_data[\"model\"] == m][\"avg_accuracy\"].values[0]\n",
    "                    if len(type_data[type_data[\"model\"] == m]) > 0\n",
    "                    else 0\n",
    "                )\n",
    "                for m in models\n",
    "            ]\n",
    "            ax.bar(\n",
    "                x + i * width,\n",
    "                values,\n",
    "                width,\n",
    "                label=t,\n",
    "                color=bar_colors[i % len(bar_colors)],\n",
    "            )\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"Average Accuracy (%)\")\n",
    "    ax.set_title(\n",
    "        \"Average Accuracy by Model (Baseline vs ThinkBrake)\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax.set_xticks(x + width * 1.5)\n",
    "    ax.set_xticklabels(models)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for this plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40de0e0",
   "metadata": {},
   "source": [
    "## Per-Model Analysis: Token Savings across Benchmarks\n",
    "Each subplot shows one model's token savings (%) at different thresholds for all benchmarks (higher = more efficient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec24b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-model token savings comparison across benchmarks\n",
    "num_models = len(filtered_rollout_data)\n",
    "if num_models == 0:\n",
    "    print(\"No data available for this plot.\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(num_models, 1, figsize=(12, 7 * num_models))\n",
    "    if num_models == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for model_idx, (rollout_name, rollout_model_data) in enumerate(\n",
    "        filtered_rollout_data.items()\n",
    "    ):\n",
    "        ax = axes[model_idx]\n",
    "        mapped_name = model_name_map.get(rollout_name, rollout_name)\n",
    "        display_name = short_names.get(mapped_name, rollout_name)\n",
    "\n",
    "        for bench_idx, benchmark in enumerate(benchmarks):\n",
    "            try:\n",
    "                if benchmark not in rollout_model_data:\n",
    "                    continue\n",
    "\n",
    "                # Baseline\n",
    "                baseline_tokens = rollout_model_data[benchmark][\"avg_token_length\"]\n",
    "\n",
    "                # ThinkBrake data\n",
    "                if (\n",
    "                    mapped_name in filtered_thinkbrake_data\n",
    "                    and benchmark in filtered_thinkbrake_data[mapped_name]\n",
    "                ):\n",
    "                    tb_data = filtered_thinkbrake_data[mapped_name][benchmark]\n",
    "                    x_vals = [\"Baseline\"]\n",
    "                    y_vals = [0]  # Baseline = 0% savings\n",
    "\n",
    "                    for thresh in thresholds:\n",
    "                        key = f\"threshold_{thresh}\"\n",
    "                        if key in tb_data:\n",
    "                            x_vals.append(f\"t={thresh}\")\n",
    "                            # Calculate token savings % (higher = more efficient)\n",
    "                            token_savings = (\n",
    "                                1 - tb_data[key][\"avg_token_length\"] / baseline_tokens\n",
    "                            ) * 100\n",
    "                            y_vals.append(token_savings)\n",
    "\n",
    "                    ax.plot(\n",
    "                        x_vals,\n",
    "                        y_vals,\n",
    "                        marker=\"s\",\n",
    "                        linewidth=2,\n",
    "                        markersize=8,\n",
    "                        color=benchmark_colors[bench_idx % len(benchmark_colors)],\n",
    "                        label=benchmark,\n",
    "                    )\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        ax.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5, linewidth=1)\n",
    "        ax.set_xlabel(\"Method\")\n",
    "        ax.set_ylabel(\"Token Savings (%)\")\n",
    "        ax.set_title(f\"{display_name}\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.legend(loc=\"best\", fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Per-Model Token Savings: Baseline vs ThinkBrake Thresholds\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "        y=1.02,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb78c9",
   "metadata": {},
   "source": [
    "## Per-Model Bar Chart: Accuracy by Benchmark\n",
    "Bar chart showing accuracy for each benchmark grouped by threshold (per model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806951a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-model bar chart: Accuracy by benchmark\n",
    "num_models = len(filtered_rollout_data)\n",
    "if num_models == 0:\n",
    "    print(\"No data available for this plot.\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(num_models, 1, figsize=(14, 7 * num_models))\n",
    "    if num_models == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    methods = [\"Baseline\"] + [f\"t={t}\" for t in thresholds]\n",
    "\n",
    "    for model_idx, (rollout_name, rollout_model_data) in enumerate(\n",
    "        filtered_rollout_data.items()\n",
    "    ):\n",
    "        ax = axes[model_idx]\n",
    "        mapped_name = model_name_map.get(rollout_name, rollout_name)\n",
    "        display_name = short_names.get(mapped_name, rollout_name)\n",
    "\n",
    "        # Collect data for this model\n",
    "        model_benchmarks = []\n",
    "        model_accuracies = {m: [] for m in methods}\n",
    "\n",
    "        for benchmark in benchmarks:\n",
    "            try:\n",
    "                if benchmark not in rollout_model_data:\n",
    "                    continue\n",
    "\n",
    "                model_benchmarks.append(benchmark)\n",
    "\n",
    "                # Baseline\n",
    "                model_accuracies[\"Baseline\"].append(\n",
    "                    rollout_model_data[benchmark][\"accuracy\"]\n",
    "                )\n",
    "\n",
    "                # ThinkBrake thresholds\n",
    "                if (\n",
    "                    mapped_name in filtered_thinkbrake_data\n",
    "                    and benchmark in filtered_thinkbrake_data[mapped_name]\n",
    "                ):\n",
    "                    tb_data = filtered_thinkbrake_data[mapped_name][benchmark]\n",
    "                    for thresh in thresholds:\n",
    "                        key = f\"threshold_{thresh}\"\n",
    "                        if key in tb_data:\n",
    "                            model_accuracies[f\"t={thresh}\"].append(\n",
    "                                tb_data[key][\"accuracy\"]\n",
    "                            )\n",
    "                        else:\n",
    "                            model_accuracies[f\"t={thresh}\"].append(0)\n",
    "                else:\n",
    "                    for t in thresholds:\n",
    "                        model_accuracies[f\"t={t}\"].append(0)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        # Plot grouped bars\n",
    "        if model_benchmarks:\n",
    "            x = np.arange(len(model_benchmarks))\n",
    "            width = 0.15\n",
    "\n",
    "            for i, method in enumerate(methods):\n",
    "                try:\n",
    "                    if model_accuracies[method]:\n",
    "                        bars = ax.bar(\n",
    "                            x + i * width,\n",
    "                            model_accuracies[method],\n",
    "                            width,\n",
    "                            label=method,\n",
    "                            color=bar_colors[i % len(bar_colors)],\n",
    "                        )\n",
    "                        # Add value labels on bars\n",
    "                        for bar, val in zip(bars, model_accuracies[method]):\n",
    "                            if val > 0:\n",
    "                                ax.text(\n",
    "                                    bar.get_x() + bar.get_width() / 2,\n",
    "                                    bar.get_height() + 0.5,\n",
    "                                    f\"{val:.1f}\",\n",
    "                                    ha=\"center\",\n",
    "                                    va=\"bottom\",\n",
    "                                    fontsize=11,\n",
    "                                    rotation=90,\n",
    "                                )\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            ax.set_xlabel(\"Benchmark\")\n",
    "            ax.set_ylabel(\"Accuracy (%)\")\n",
    "            ax.set_title(f\"{display_name}\", fontsize=14, fontweight=\"bold\")\n",
    "            ax.set_xticks(x + width * 3.5)\n",
    "            ax.set_xticklabels(model_benchmarks, ha=\"right\")\n",
    "            ax.legend(loc=\"upper right\", fontsize=8)\n",
    "            ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "            ax.set_ylim(0, 105)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412f117",
   "metadata": {},
   "source": [
    "## Method Comparison: ThinkBrake vs Thinkless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../outputs/leaderboard_thinkless.json\", \"r\") as f:\n",
    "        thinkless_data = json.load(f)\n",
    "\n",
    "    filtered_thinkless_data = {}\n",
    "    for model_key in INCLUDE_MODELS:\n",
    "        if model_key in thinkless_data:\n",
    "            filtered_thinkless_data[model_key] = thinkless_data[model_key]\n",
    "        mapped_name = model_name_map.get(model_key, model_key)\n",
    "        if mapped_name in thinkless_data:\n",
    "            filtered_thinkless_data[model_key] = thinkless_data[mapped_name]\n",
    "except FileNotFoundError:\n",
    "    filtered_thinkless_data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b008cde4",
   "metadata": {},
   "source": [
    "### Accuracy Comparison: Baseline vs Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c88432",
   "metadata": {},
   "outputs": [],
   "source": [
    "if filtered_thinkless_data:\n",
    "    best_threshold = \"1.0\"\n",
    "    n_benchmarks = len(benchmarks)\n",
    "    fig, axes = create_dynamic_subplots(n_benchmarks, max_cols=3)\n",
    "\n",
    "    method_colors = {\n",
    "        \"Baseline\": \"#4a4a4a\",\n",
    "        \"Thinkless\": \"#e74c3c\",\n",
    "        \"ThinkBrake\": \"#3498db\",\n",
    "    }\n",
    "\n",
    "    for idx, benchmark in enumerate(benchmarks):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        models_list = []\n",
    "        baseline_accs = []\n",
    "        thinkless_accs = []\n",
    "        thinkbrake_accs = []\n",
    "\n",
    "        for rollout_name in filtered_rollout_data:\n",
    "            mapped_name = model_name_map.get(rollout_name, rollout_name)\n",
    "            display_name = short_names.get(mapped_name, rollout_name)\n",
    "\n",
    "            try:\n",
    "                # Baseline accuracy\n",
    "                if benchmark in filtered_rollout_data[rollout_name]:\n",
    "                    baseline_acc = filtered_rollout_data[rollout_name][benchmark][\n",
    "                        \"accuracy\"\n",
    "                    ]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                # Thinkless accuracy\n",
    "                thinkless_acc = None\n",
    "                if (\n",
    "                    rollout_name in filtered_thinkless_data\n",
    "                    and benchmark in filtered_thinkless_data[rollout_name]\n",
    "                ):\n",
    "                    thinkless_acc = filtered_thinkless_data[rollout_name][benchmark][\n",
    "                        \"accuracy\"\n",
    "                    ]\n",
    "\n",
    "                # ThinkBrake accuracy (best threshold)\n",
    "                thinkbrake_acc = None\n",
    "                if (\n",
    "                    mapped_name in filtered_thinkbrake_data\n",
    "                    and benchmark in filtered_thinkbrake_data[mapped_name]\n",
    "                ):\n",
    "                    key = f\"threshold_{best_threshold}\"\n",
    "                    if key in filtered_thinkbrake_data[mapped_name][benchmark]:\n",
    "                        thinkbrake_acc = filtered_thinkbrake_data[mapped_name][\n",
    "                            benchmark\n",
    "                        ][key][\"accuracy\"]\n",
    "\n",
    "                models_list.append(display_name)\n",
    "                baseline_accs.append(baseline_acc)\n",
    "                thinkless_accs.append(thinkless_acc if thinkless_acc else 0)\n",
    "                thinkbrake_accs.append(thinkbrake_acc if thinkbrake_acc else 0)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if models_list:\n",
    "            x = np.arange(len(models_list))\n",
    "            width = 0.25\n",
    "\n",
    "            ax.bar(\n",
    "                x - width,\n",
    "                baseline_accs,\n",
    "                width,\n",
    "                label=\"Baseline\",\n",
    "                color=method_colors[\"Baseline\"],\n",
    "            )\n",
    "            ax.bar(\n",
    "                x,\n",
    "                thinkless_accs,\n",
    "                width,\n",
    "                label=\"Thinkless\",\n",
    "                color=method_colors[\"Thinkless\"],\n",
    "            )\n",
    "            ax.bar(\n",
    "                x + width,\n",
    "                thinkbrake_accs,\n",
    "                width,\n",
    "                label=f\"ThinkBrake (t={best_threshold})\",\n",
    "                color=method_colors[\"ThinkBrake\"],\n",
    "            )\n",
    "\n",
    "            ax.set_xlabel(\"Model\")\n",
    "            ax.set_ylabel(\"Accuracy (%)\")\n",
    "            ax.set_title(f\"{benchmark}\", fontsize=12, fontweight=\"bold\")\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(models_list, rotation=45, ha=\"right\", fontsize=9)\n",
    "            ax.legend(loc=\"best\", fontsize=8)\n",
    "            ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "            ax.set_ylim(0, 105)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e37e8e4",
   "metadata": {},
   "source": [
    "### Token Efficiency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b116e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if filtered_thinkless_data:\n",
    "    best_threshold = \"1.0\"\n",
    "\n",
    "    n_benchmarks = len(benchmarks)\n",
    "    fig, axes = create_dynamic_subplots(n_benchmarks, max_cols=3)\n",
    "\n",
    "    method_colors = {\"Thinkless\": \"#e74c3c\", \"ThinkBrake\": \"#3498db\"}\n",
    "\n",
    "    for idx, benchmark in enumerate(benchmarks):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        models_list = []\n",
    "        thinkless_savings = []\n",
    "        thinkbrake_savings = []\n",
    "\n",
    "        for rollout_name in filtered_rollout_data:\n",
    "            mapped_name = model_name_map.get(rollout_name, rollout_name)\n",
    "            display_name = short_names.get(mapped_name, rollout_name)\n",
    "\n",
    "            try:\n",
    "                # Baseline token length\n",
    "                if benchmark in filtered_rollout_data[rollout_name]:\n",
    "                    baseline_tokens = filtered_rollout_data[rollout_name][benchmark][\n",
    "                        \"avg_token_length\"\n",
    "                    ]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                # Thinkless token savings\n",
    "                thinkless_saving = None\n",
    "                if (\n",
    "                    rollout_name in filtered_thinkless_data\n",
    "                    and benchmark in filtered_thinkless_data[rollout_name]\n",
    "                ):\n",
    "                    thinkless_tokens = filtered_thinkless_data[rollout_name][benchmark][\n",
    "                        \"avg_token_length\"\n",
    "                    ]\n",
    "                    thinkless_saving = (1 - thinkless_tokens / baseline_tokens) * 100\n",
    "\n",
    "                # ThinkBrake token savings\n",
    "                thinkbrake_saving = None\n",
    "                if (\n",
    "                    mapped_name in filtered_thinkbrake_data\n",
    "                    and benchmark in filtered_thinkbrake_data[mapped_name]\n",
    "                ):\n",
    "                    key = f\"threshold_{best_threshold}\"\n",
    "                    if key in filtered_thinkbrake_data[mapped_name][benchmark]:\n",
    "                        thinkbrake_tokens = filtered_thinkbrake_data[mapped_name][\n",
    "                            benchmark\n",
    "                        ][key][\"avg_token_length\"]\n",
    "                        thinkbrake_saving = (\n",
    "                            1 - thinkbrake_tokens / baseline_tokens\n",
    "                        ) * 100\n",
    "\n",
    "                models_list.append(display_name)\n",
    "                thinkless_savings.append(thinkless_saving if thinkless_saving else 0)\n",
    "                thinkbrake_savings.append(thinkbrake_saving if thinkbrake_saving else 0)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if models_list:\n",
    "            x = np.arange(len(models_list))\n",
    "            width = 0.35\n",
    "\n",
    "            ax.bar(\n",
    "                x - width / 2,\n",
    "                thinkless_savings,\n",
    "                width,\n",
    "                label=\"Thinkless\",\n",
    "                color=method_colors[\"Thinkless\"],\n",
    "            )\n",
    "            ax.bar(\n",
    "                x + width / 2,\n",
    "                thinkbrake_savings,\n",
    "                width,\n",
    "                label=f\"ThinkBrake (t={best_threshold})\",\n",
    "                color=method_colors[\"ThinkBrake\"],\n",
    "            )\n",
    "\n",
    "            ax.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "            ax.set_xlabel(\"Model\")\n",
    "            ax.set_ylabel(\"Token Savings (%)\")\n",
    "            ax.set_title(f\"{benchmark}\", fontsize=12, fontweight=\"bold\")\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(models_list, rotation=45, ha=\"right\", fontsize=9)\n",
    "            ax.legend(loc=\"best\", fontsize=8)\n",
    "            ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
