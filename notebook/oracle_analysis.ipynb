{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a55cbe",
   "metadata": {},
   "source": [
    "# ThinkBrake: Oracle Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f01a69",
   "metadata": {},
   "source": [
    "## Import and Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c0b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"THINKBRAKE_ROOT\"] = \"/home/work/ThinkBrake\"\n",
    "sys.path.insert(0, \"/home/work/ThinkBrake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a47f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from math_verify import parse, verify\n",
    "\n",
    "from thinkbrake.func.constants import (\n",
    "    RESULT_DIR,\n",
    "    ORACLE_PREFIX,\n",
    "    ROLLOUT_PREFIX,\n",
    ")\n",
    "from thinkbrake.func.utils import get_parent_category\n",
    "\n",
    "print(f\"Result directory: {RESULT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d175e66",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_MODELS = [\"Qwen/Qwen3-4B-Thinking-2507\"]\n",
    "TARGET_CATEGORIES = [\"aime2024\", \"aime2025\"]\n",
    "\n",
    "TARGET_MODELS = [s.replace(\"/\", \"_\") for s in TARGET_MODELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a330fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_oracle_file(file_path: str, sub_category: str = None) -> dict:\n",
    "    results_by_problem = {}\n",
    "    parent_category = get_parent_category(sub_category) if sub_category else None\n",
    "    is_multiple_choice = parent_category == \"general\"\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line.strip())\n",
    "            problem_id = item[\"id\"]\n",
    "            sentence_idx = item.get(\"sentence_idx\", 0)\n",
    "\n",
    "            if sentence_idx == 0:\n",
    "                continue\n",
    "\n",
    "            if problem_id not in results_by_problem:\n",
    "                results_by_problem[problem_id] = {\n",
    "                    \"ground_truth\": item[\"answer\"],\n",
    "                    \"oracle_hit\": False,\n",
    "                    \"correct_sentence_indices\": [],\n",
    "                    \"total_sentences\": 0,\n",
    "                    \"first_correct_idx\": None,\n",
    "                    \"token_lengths\": [],\n",
    "                }\n",
    "\n",
    "            results_by_problem[problem_id][\"total_sentences\"] += 1\n",
    "            response = item.get(\"response\", \"\")\n",
    "            token_length = item.get(\"token_length\", len(response.split()))\n",
    "            results_by_problem[problem_id][\"token_lengths\"].append(token_length)\n",
    "\n",
    "            try:\n",
    "                if is_multiple_choice:\n",
    "                    ground_truth = item[\"answer\"]\n",
    "                    predicted = item.get(\"response\", \"\")\n",
    "                    patterns = [\n",
    "                        r'[\"\\*]*answer[\"\\*]*\\s*[:=]\\s*[\"\\']?([A-Da-d])[\"\\']?',\n",
    "                        r\"(?:the\\s+)?answer\\s+is[:\\s]*([A-Da-d])\\b\",\n",
    "                        r\"\\b([A-Da-d])\\s*$\",\n",
    "                    ]\n",
    "                    predicted_answer = \"\"\n",
    "                    for pattern in patterns:\n",
    "                        matches = re.findall(pattern, predicted, re.IGNORECASE)\n",
    "                        if matches:\n",
    "                            predicted_answer = matches[-1].upper()\n",
    "                            break\n",
    "                    is_correct = (\n",
    "                        ground_truth.upper().strip() == predicted_answer.upper().strip()\n",
    "                    )\n",
    "                else:\n",
    "                    ground_truth = parse(f\"${item['answer']}$\")\n",
    "                    predicted = parse(response)\n",
    "                    is_correct = verify(ground_truth, predicted)\n",
    "\n",
    "                if is_correct:\n",
    "                    results_by_problem[problem_id][\"oracle_hit\"] = True\n",
    "                    results_by_problem[problem_id][\"correct_sentence_indices\"].append(\n",
    "                        sentence_idx\n",
    "                    )\n",
    "                    if results_by_problem[problem_id][\"first_correct_idx\"] is None:\n",
    "                        results_by_problem[problem_id][\n",
    "                            \"first_correct_idx\"\n",
    "                        ] = sentence_idx\n",
    "\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return results_by_problem\n",
    "\n",
    "\n",
    "def aggregate_oracle_results(results_by_problem: dict) -> dict:\n",
    "    total = len(results_by_problem)\n",
    "    oracle_hits = sum(1 for r in results_by_problem.values() if r[\"oracle_hit\"])\n",
    "\n",
    "    first_correct_tokens = []\n",
    "    for _, r in results_by_problem.items():\n",
    "        if r[\"first_correct_idx\"] is not None and r[\"first_correct_idx\"] < len(\n",
    "            r[\"token_lengths\"]\n",
    "        ):\n",
    "            first_correct_tokens.append(r[\"token_lengths\"][r[\"first_correct_idx\"]])\n",
    "\n",
    "    total_tokens = []\n",
    "    for r in results_by_problem.values():\n",
    "        if r[\"token_lengths\"]:\n",
    "            total_tokens.append(max(r[\"token_lengths\"]))\n",
    "\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"oracle_hits\": oracle_hits,\n",
    "        \"oracle_accuracy\": (oracle_hits / total * 100) if total > 0 else 0.0,\n",
    "        \"avg_first_correct_tokens\": (\n",
    "            sum(first_correct_tokens) / len(first_correct_tokens)\n",
    "            if first_correct_tokens\n",
    "            else 0.0\n",
    "        ),\n",
    "        \"avg_total_tokens\": (\n",
    "            sum(total_tokens) / len(total_tokens) if total_tokens else 0.0\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5cf86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_oracle_results(\n",
    "    target_models: list = None, target_categories: list = None\n",
    ") -> dict:\n",
    "    all_results = {}\n",
    "\n",
    "    for model_dir in RESULT_DIR.iterdir():\n",
    "        if not model_dir.is_dir() or model_dir.name.startswith(\"leaderboard\"):\n",
    "            continue\n",
    "\n",
    "        model_name = model_dir.name\n",
    "\n",
    "        if target_models is not None and model_name not in target_models:\n",
    "            continue\n",
    "\n",
    "        all_results[model_name] = {}\n",
    "\n",
    "        for parent_cat_dir in model_dir.iterdir():\n",
    "            if not parent_cat_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            oracle_dir = parent_cat_dir / ORACLE_PREFIX\n",
    "            if not oracle_dir.exists():\n",
    "                continue\n",
    "\n",
    "            for result_file in oracle_dir.glob(\"*_result.jsonl\"):\n",
    "                sub_category = result_file.stem.replace(\"_result\", \"\")\n",
    "\n",
    "                if (\n",
    "                    target_categories is not None\n",
    "                    and sub_category not in target_categories\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    results_by_problem = evaluate_oracle_file(\n",
    "                        str(result_file), sub_category\n",
    "                    )\n",
    "                    aggregated = aggregate_oracle_results(results_by_problem)\n",
    "                    all_results[model_name][sub_category] = {\n",
    "                        \"detailed\": results_by_problem,\n",
    "                        \"summary\": aggregated,\n",
    "                    }\n",
    "                    print(f\"Processed: {model_name} / {sub_category}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {model_name}: {e}\")\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ffe46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_results = collect_oracle_results(\n",
    "    target_models=TARGET_MODELS, target_categories=TARGET_CATEGORIES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c552f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rollout_file(file_path: str, sub_category: str = None) -> dict:\n",
    "    parent_category = get_parent_category(sub_category) if sub_category else None\n",
    "    is_multiple_choice = parent_category == \"general\"\n",
    "\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line.strip())\n",
    "\n",
    "            try:\n",
    "                if is_multiple_choice:\n",
    "                    ground_truth = item[\"answer\"]\n",
    "                    predicted = item.get(\"response\", \"\")\n",
    "                    patterns = [\n",
    "                        r'[\"\\*]*answer[\"\\*]*\\s*[:=]\\s*[\"\\']?([A-Da-d])[\"\\']?',\n",
    "                        r\"(?:the\\s+)?answer\\s+is[:\\s]*([A-Da-d])\\b\",\n",
    "                        r\"\\b([A-Da-d])\\s*$\",\n",
    "                    ]\n",
    "                    predicted_answer = \"\"\n",
    "                    for pattern in patterns:\n",
    "                        matches = re.findall(pattern, predicted, re.IGNORECASE)\n",
    "                        if matches:\n",
    "                            predicted_answer = matches[-1].upper()\n",
    "                            break\n",
    "                    is_correct = (\n",
    "                        ground_truth.upper().strip() == predicted_answer.upper().strip()\n",
    "                    )\n",
    "                else:\n",
    "                    ground_truth = parse(f\"${item['answer']}$\")\n",
    "                    predicted = parse(item.get(\"response\", \"\"))\n",
    "                    is_correct = verify(ground_truth, predicted)\n",
    "\n",
    "                if is_correct:\n",
    "                    correct_count += 1\n",
    "\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            total_count += 1\n",
    "            total_tokens += item.get(\"token_length\", 0)\n",
    "\n",
    "    accuracy = (correct_count / total_count * 100) if total_count > 0 else 0.0\n",
    "    avg_token_length = (total_tokens / total_count) if total_count > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"total\": total_count,\n",
    "        \"correct\": correct_count,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"avg_token_length\": avg_token_length,\n",
    "    }\n",
    "\n",
    "\n",
    "def collect_rollout_results(\n",
    "    target_models: list = None, target_categories: list = None\n",
    ") -> dict:\n",
    "    all_results = {}\n",
    "\n",
    "    for model_dir in RESULT_DIR.iterdir():\n",
    "        if not model_dir.is_dir() or model_dir.name.startswith(\"leaderboard\"):\n",
    "            continue\n",
    "\n",
    "        model_name = model_dir.name\n",
    "\n",
    "        if target_models is not None and model_name not in target_models:\n",
    "            continue\n",
    "\n",
    "        all_results[model_name] = {}\n",
    "\n",
    "        for parent_cat_dir in model_dir.iterdir():\n",
    "            if not parent_cat_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            rollout_dir = parent_cat_dir / ROLLOUT_PREFIX\n",
    "            if not rollout_dir.exists():\n",
    "                continue\n",
    "\n",
    "            for result_file in rollout_dir.glob(\"*_result.jsonl\"):\n",
    "                sub_category = result_file.stem.replace(\"_result\", \"\")\n",
    "\n",
    "                if (\n",
    "                    target_categories is not None\n",
    "                    and sub_category not in target_categories\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    result = evaluate_rollout_file(str(result_file), sub_category)\n",
    "                    all_results[model_name][sub_category] = result\n",
    "                    print(f\"Processed rollout: {model_name} / {sub_category}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing rollout {model_name}/{sub_category}: {e}\")\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_data = collect_rollout_results(\n",
    "    target_models=TARGET_MODELS, target_categories=TARGET_CATEGORIES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f62ac9",
   "metadata": {},
   "source": [
    "## Evaluate and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b26b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = []\n",
    "\n",
    "for model_name, categories in oracle_results.items():\n",
    "    for category, result in categories.items():\n",
    "        summary = result[\"summary\"]\n",
    "        rollout_cat_data = rollout_data.get(model_name, {}).get(category, None)\n",
    "\n",
    "        comparison_data.append(\n",
    "            {\n",
    "                \"Model\": model_name,\n",
    "                \"Category\": category,\n",
    "                \"Oracle Accuracy (%)\": round(summary[\"oracle_accuracy\"], 2),\n",
    "                \"Oracle Hits\": summary[\"oracle_hits\"],\n",
    "                \"Total Problems\": summary[\"total\"],\n",
    "                \"Rollout Accuracy (%)\": (\n",
    "                    round(rollout_cat_data[\"accuracy\"], 2) if rollout_cat_data else None\n",
    "                ),\n",
    "                \"Rollout Correct\": (\n",
    "                    rollout_cat_data[\"correct\"] if rollout_cat_data else None\n",
    "                ),\n",
    "                \"Avg First Correct Tokens\": round(\n",
    "                    summary[\"avg_first_correct_tokens\"], 0\n",
    "                ),\n",
    "                \"Rollout Avg Tokens\": (\n",
    "                    round(rollout_cat_data[\"avg_token_length\"], 0)\n",
    "                    if rollout_cat_data\n",
    "                    else None\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "df_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cccacae",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e437ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_oracle_leaderboard(oracle_results: dict, rollout_data: dict):\n",
    "    leaderboard_data = []\n",
    "\n",
    "    for model_name, categories in oracle_results.items():\n",
    "        if not categories:\n",
    "            continue\n",
    "        for category, result in categories.items():\n",
    "            summary = result[\"summary\"]\n",
    "            rollout_cat_data = rollout_data.get(model_name, {}).get(category, None)\n",
    "\n",
    "            leaderboard_data.append(\n",
    "                {\n",
    "                    \"Model\": model_name,\n",
    "                    \"Category\": category,\n",
    "                    \"Oracle Accuracy\": round(summary[\"oracle_accuracy\"], 2),\n",
    "                    \"Rollout Accuracy\": (\n",
    "                        round(rollout_cat_data[\"accuracy\"], 2)\n",
    "                        if rollout_cat_data\n",
    "                        else None\n",
    "                    ),\n",
    "                    \"Avg Oracle Tokens\": round(summary[\"avg_first_correct_tokens\"], 2),\n",
    "                    \"Rollout Avg Tokens\": (\n",
    "                        round(rollout_cat_data[\"avg_token_length\"], 2)\n",
    "                        if rollout_cat_data\n",
    "                        else None\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df_leaderboard = pd.DataFrame(leaderboard_data)\n",
    "    output_file = RESULT_DIR / f\"leaderboard_{ORACLE_PREFIX}.csv\"\n",
    "    df_leaderboard.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"Saved oracle leaderboard to: {output_file}\")\n",
    "    return df_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5516d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_leaderboard = save_oracle_leaderboard(oracle_results, rollout_data)\n",
    "oracle_leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e3087d",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a95d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plot_df = df_comparison.copy()\n",
    "\n",
    "categories = plot_df[\"Category\"].unique().tolist()\n",
    "models = plot_df[\"Model\"].unique().tolist()\n",
    "n_cat = len(categories)\n",
    "\n",
    "x = np.arange(n_cat)\n",
    "bar_width = 0.35\n",
    "max_acc = 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharex=True)\n",
    "\n",
    "# Assume only one model\n",
    "model = models[0]\n",
    "model_df = plot_df[plot_df[\"Model\"] == model].set_index(\"Category\")\n",
    "\n",
    "# Subplot 1: Rollout vs Oracle Accuracy (Rollout left, Oracle right)\n",
    "rollout_vals = [\n",
    "    model_df.loc[c, \"Rollout Accuracy (%)\"] if c in model_df.index else 0\n",
    "    for c in categories\n",
    "]\n",
    "oracle_vals = [\n",
    "    model_df.loc[c, \"Oracle Accuracy (%)\"] if c in model_df.index else 0\n",
    "    for c in categories\n",
    "]\n",
    "max_bar1 = max(rollout_vals + oracle_vals + [max_acc])\n",
    "bars1 = axes[0].bar(\n",
    "    x - bar_width / 2, rollout_vals, width=bar_width, label=\"Rollout\", alpha=0.8\n",
    ")\n",
    "bars2 = axes[0].bar(\n",
    "    x + bar_width / 2, oracle_vals, width=bar_width, label=\"Oracle\", alpha=0.6\n",
    ")\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(categories)\n",
    "axes[0].set_ylim(0, max_bar1 * 1.15)\n",
    "axes[0].set_ylabel(\"Accuracy (%)\")\n",
    "axes[0].set_title(f\"{model} - Accuracy\")\n",
    "axes[0].grid(axis=\"y\", linestyle=\":\", alpha=0.5)\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[0].annotate(\n",
    "        f\"{height:.1f}\",\n",
    "        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "        xytext=(0, 5),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[0].annotate(\n",
    "        f\"{height:.1f}\",\n",
    "        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "        xytext=(0, 5),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "# Subplot 2: Rollout vs Oracle Avg Tokens (Rollout left, Oracle right)\n",
    "rollout_tokens = [\n",
    "    model_df.loc[c, \"Rollout Avg Tokens\"] if c in model_df.index else 0\n",
    "    for c in categories\n",
    "]\n",
    "oracle_tokens = [\n",
    "    model_df.loc[c, \"Avg First Correct Tokens\"] if c in model_df.index else 0\n",
    "    for c in categories\n",
    "]\n",
    "max_tokens = max(max(oracle_tokens), max(rollout_tokens), 100)\n",
    "max_bar2 = max(rollout_tokens + oracle_tokens + [max_tokens])\n",
    "bars3 = axes[1].bar(\n",
    "    x - bar_width / 2, rollout_tokens, width=bar_width, label=\"Rollout\", alpha=0.8\n",
    ")\n",
    "bars4 = axes[1].bar(\n",
    "    x + bar_width / 2, oracle_tokens, width=bar_width, label=\"Oracle\", alpha=0.6\n",
    ")\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(categories)\n",
    "axes[1].set_ylim(0, max_bar2 * 1.15)\n",
    "axes[1].set_ylabel(\"Avg Tokens\")\n",
    "axes[1].set_title(f\"{model} - Avg Tokens\")\n",
    "axes[1].grid(axis=\"y\", linestyle=\":\", alpha=0.5)\n",
    "# Add value labels\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    axes[1].annotate(\n",
    "        f\"{height:.1f}\",\n",
    "        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "        xytext=(0, 5),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "for bar in bars4:\n",
    "    height = bar.get_height()\n",
    "    axes[1].annotate(\n",
    "        f\"{height:.1f}\",\n",
    "        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "        xytext=(0, 5),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "# Move legend to the bottom center\n",
    "handles, labels = axes[1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"lower center\", bbox_to_anchor=(0.5, -0.08), ncol=2)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
